{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayush-mishra-0-0/ML/blob/main/ayushKumarMishra_12240340_nlp_assign2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <h1 align=\"center\"> DS504: NLP - Lab</h1>\n",
        "<h1 align=\"center\">Assignment - 2</h1>\n",
        "<h1 align=\"center\">27/08/2024</h1>\n",
        "\n",
        "---\n",
        "<font color=\"lightgreen\">**Name:** Ayush Kumar Mishra <br>\n",
        "**Roll No:** 12240340 </font>"
      ],
      "metadata": {
        "id": "_Grt4MzOZ4_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Questions**\n"
      ],
      "metadata": {
        "id": "KAHG_psOk2pb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q1) Write a program that parses a given word into its root form and affixes (prefixes and suffixes). The program should output the morphemes.**\n",
        "Example Input: \"unhappiness\"\n",
        "\n",
        "Example Output: [\"un-\", \"happy\", \"-ness\"]"
      ],
      "metadata": {
        "id": "FgF-CUm1le5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the `solve` Function\n",
        "\n",
        "1. **<span style=\"color: blue;\">Prefix and Suffix Lists</span>**:\n",
        "   - The function defines two lists: `prefixes` and `suffixes`.\n",
        "   - <span style=\"color: green;\">**Prefixes**</span>: `['un', 're', 'in', 'dis', 'im', 'ir', 'il']`\n",
        "   - <span style=\"color: green;\">**Suffixes**</span>: `['ed', 'ing', 'ly', 'ness', 's', 'er', 'able', 'ment']`\n",
        "   - These lists contain common prefixes and suffixes used for word transformation.\n",
        "\n",
        "2. **<span style=\"color: blue;\">Prefix Removal</span>**:\n",
        "   - The function checks if the input `word` starts with any of the prefixes from the `prefixes` list.\n",
        "   - If a match is found, the prefix is removed from the word, and the prefix is printed.\n",
        "\n",
        "3. **<span style=\"color: blue;\">Suffix Removal</span>**:\n",
        "   - After removing a prefix (if any), the function checks if the remaining base word ends with any of the suffixes from the `suffixes` list.\n",
        "   - If a match is found, the suffix is removed from the word, and the suffix is printed.\n",
        "\n",
        "4. **<span style=\"color: blue;\">Special Case Handling</span>**:\n",
        "   - The function includes a special case where if the base word ends with 'i' and the original word ends with 'ness', the 'i' is replaced with 'y'.\n",
        "   - This handles words like \"happiness\" where the base form is \"happy\".\n",
        "\n",
        "5. **<span style=\"color: blue;\">Output</span>**:\n",
        "   - Finally, the function prints the resulting base word after processing the prefixes, suffixes, and special cases.\n"
      ],
      "metadata": {
        "id": "Qx6qoXTJ7O6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def solve(word):\n",
        "    prefixes = ['un', 're', 'in', 'dis', 'im', 'ir', 'il']\n",
        "    suffixes = ['ed', 'ing', 'ly', 'ness', 's', 'er', 'able', 'ment']\n",
        "\n",
        "    base_word = word\n",
        "\n",
        "    for prefix in prefixes:\n",
        "        if word.startswith(prefix):\n",
        "            base_word = word[len(prefix):]\n",
        "            print(f\"Prefix: {prefix}\")\n",
        "            break\n",
        "\n",
        "    for suffix in suffixes:\n",
        "        if base_word.endswith(suffix):\n",
        "            base_word = base_word[:-len(suffix)]\n",
        "            print(f\"Suffix: {suffix}\")\n",
        "            break\n",
        "\n",
        "    if base_word.endswith('i') and word.endswith('ness'):\n",
        "        base_word = base_word[:-1] + 'y'\n",
        "\n",
        "    print(f\"Base Word: {base_word}\")\n",
        "\n",
        "# Example usage\n",
        "solve(\"unhappiness\")"
      ],
      "metadata": {
        "id": "JWPC2TczmrHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966d8dcd-0c39-4bad-a91a-c569f9295d56"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prefix: un\n",
            "Suffix: ness\n",
            "Base Word: happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q2) Write a program that identifies the derivational morphemes in a given sentence and classifies each word into its base form and derivational affixes.**\n",
        "\n",
        "Example:\n",
        "Input Sentence: \"The artistically gifted boy performed exceptionally in the competition.\"\n",
        "\n",
        "Output:\n",
        "\n",
        "    \"word\": \"artistically\",\n",
        "    \"base_form\": \"art\",\n",
        "    \"derivational_affixes\": [\"-ist\", \"-ic\", \"-ally\"],\n",
        "  \n",
        "    \"word\": \"gifted\",\n",
        "    \"base_form\": \"gift\",\n",
        "    \"derivational_affixes\": [\"-ed\"],\n",
        "  \n",
        "    \"word\": \"performed\",\n",
        "    \"base_form\": \"perform\",\n",
        "    \"derivational_affixes\": [\"-ed\"],\n",
        "  \n",
        "    \"word\": \"exceptionally\",\n",
        "    \"base_form\": \"except\",\n",
        "    \"derivational_affixes\": [\"-ion\", \"-al\", \"-ly\"],\n",
        "  \n",
        "    \"word\": \"competition\",\n",
        "    \"base_form\": \"compete\",\n",
        "    \"derivational_affixes\": [\"-ition\"]\n"
      ],
      "metadata": {
        "id": "Avb1UNkPme6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the `DerivationalMorphemeAnalyzer` Class\n",
        "\n",
        "1. **<span style=\"color: blue;\">Initialization</span>**:\n",
        "   - The class `DerivationalMorphemeAnalyzer` initializes with a `WordNetLemmatizer` from NLTK for lemmatization.\n",
        "   - It defines lists of common <span style=\"color: green;\">prefixes</span> and <span style=\"color: green;\">suffixes</span> used for derivational morphology.\n",
        "\n",
        "2. **<span style=\"color: blue;\">Word Analysis</span>**:\n",
        "   - The `analyze_word` method processes a word to identify and remove prefixes and suffixes.\n",
        "   - **Prefix Removal**: Checks if the word starts with any prefix from the `prefixes` list and removes it if found.\n",
        "   - **Suffix Removal**: Iteratively checks if the word ends with any suffix from the `suffixes` list and removes it if found.\n",
        "   - The method uses the `WordNetLemmatizer` to obtain the base form of the word, which is returned along with identified affixes.\n",
        "\n",
        "3. **<span style=\"color: blue;\">Sentence Analysis</span>**:\n",
        "   - The `analyze_sentence` method tokenizes the input sentence into words.\n",
        "   - It then applies `analyze_word` to each word (excluding words shorter than 3 characters) and collects the results.\n",
        "\n",
        "4. **<span style=\"color: blue;\">Example Usage</span>**:\n",
        "   - An instance of `DerivationalMorphemeAnalyzer` is created.\n",
        "   - The `analyze_sentence` method is called with the input sentence: `\"The artistically gifted boy performed exceptionally in the competition.\"`\n",
        "   - The results are printed, showing each word's original form, its base form, and any derivational affixes identified.\n",
        "\n",
        "5. **<span style=\"color: blue;\">Output</span>**:\n",
        "   - The output will include detailed information for each word in the sentence, displaying the word's base form and any affixes removed, providing insight into the word's derivational morphology.\n"
      ],
      "metadata": {
        "id": "CaGhoAH17aFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "class DerivationalMorphemeAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.derivational_affixes = {\n",
        "            'prefixes': ['un', 're', 'dis', 'over', 'under', 'pre', 'post', 'non', 'anti', 'semi', 'mis'],\n",
        "            'suffixes': ['ness', 'less', 'ful', 'able', 'ible', 'al', 'ial', 'ic', 'ical', 'ed', 'en', 'er', 'est',\n",
        "                         'ify', 'ize', 'ise', 'ing', 'ion', 'ation', 'ition', 'ity', 'ty', 'ous', 'ious', 'ive',\n",
        "                         'ative', 'itive', 'ly', 'ward', 'wise', 'ist', 'ism', 'ship', 'able', 'ible', 'ary', 'ory',\n",
        "                         'ery', 'ally']\n",
        "        }\n",
        "\n",
        "    def analyze_word(self, word):\n",
        "        original_word = word\n",
        "        base_form = self.lemmatizer.lemmatize(word)\n",
        "        derivational_affixes = []\n",
        "\n",
        "        for prefix in self.derivational_affixes['prefixes']:\n",
        "            if word.startswith(prefix):\n",
        "                derivational_affixes.append(prefix)\n",
        "                word = word[len(prefix):]\n",
        "                break\n",
        "\n",
        "        while True:\n",
        "            found_suffix = False\n",
        "            for suffix in self.derivational_affixes['suffixes']:\n",
        "                if word.endswith(suffix) and len(word) > len(suffix):\n",
        "                    derivational_affixes.append(suffix)\n",
        "                    word = word[:-len(suffix)]\n",
        "                    found_suffix = True\n",
        "                    break\n",
        "            if not found_suffix:\n",
        "                break\n",
        "\n",
        "        if not derivational_affixes:\n",
        "            base_form = word\n",
        "\n",
        "        return {\n",
        "            \"word\": original_word,\n",
        "            \"base_form\": base_form,\n",
        "            \"derivational_affixes\": derivational_affixes\n",
        "        }\n",
        "\n",
        "    def analyze_sentence(self, sentence):\n",
        "        words = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
        "        return [self.analyze_word(word) for word in words if len(word) > 2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veigY4DzH7I9",
        "outputId": "8c783740-55fd-43a5-de19-e227ff906f0e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "analyzer = DerivationalMorphemeAnalyzer()\n",
        "input_sentence = \"The artistically gifted boy performed exceptionally in the competition.\"\n",
        "result = analyzer.analyze_sentence(input_sentence)\n",
        "\n",
        "for word_analysis in result:\n",
        "    print(word_analysis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p6iieEG7XRp",
        "outputId": "3a71d0f1-5d96-4138-bee6-b5c7d0e46b08"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'word': 'the', 'base_form': 'the', 'derivational_affixes': []}\n",
            "{'word': 'artistically', 'base_form': 'artistically', 'derivational_affixes': ['ly', 'al', 'ic', 'ist']}\n",
            "{'word': 'gifted', 'base_form': 'gifted', 'derivational_affixes': ['ed']}\n",
            "{'word': 'boy', 'base_form': 'boy', 'derivational_affixes': []}\n",
            "{'word': 'performed', 'base_form': 'performed', 'derivational_affixes': ['ed']}\n",
            "{'word': 'exceptionally', 'base_form': 'exceptionally', 'derivational_affixes': ['ly', 'al', 'ion']}\n",
            "{'word': 'the', 'base_form': 'the', 'derivational_affixes': []}\n",
            "{'word': 'competition', 'base_form': 'competition', 'derivational_affixes': ['ion']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q3) Implement a word segmentation program that can handle ambiguous cases where multiple segmentations are possible. The program should use a probabilistic model to choose the most likely segmentation.**\n",
        "Example Input: \"therapistfinder\"\n",
        "\n",
        "Example Output: [\"therapist\", \"finder\"] or [\"the\", \"rapist\", \"finder\"] (depending on context)"
      ],
      "metadata": {
        "id": "dR657un_o4sD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Code for Multiple Text Corpora\n",
        "\n",
        "1. **<span style=\"color: blue;\">Text Preprocessing</span>**:\n",
        "   - The `preprocess_text` function prepares the text by:\n",
        "     - **Converting to Lowercase**: Transforms all characters in the text to lowercase to ensure uniformity.\n",
        "     - **Removing Non-Alphanumeric Characters**: Utilizes regular expressions to strip out any characters that are not letters or spaces. This helps in focusing on the actual words.\n",
        "\n",
        "2. **<span style=\"color: blue;\">Generating One-Gram Files</span>**:\n",
        "   - The `generate_one_gram_file` function:\n",
        "     - **Processes the Text**: Applies the `preprocess_text` function to the input text to clean and prepare it.\n",
        "     - **Tokenizes and Counts Words**: Splits the processed text into words and counts the frequency of each word using the `Counter` class.\n",
        "     - **Writes to Output File**: Saves the word counts to a specified file, with each line containing a word and its count, separated by a tab.\n",
        "\n",
        "3. **<span style=\"color: blue;\">Using the One-Gram Distinction</span>**:\n",
        "   - The `OneGramDist` class:\n",
        "     - **Initializes from File**: Reads word frequencies from a file and calculates the total number of words.\n",
        "     - **Calculates Probability**: Computes the probability of a word by dividing its count by the total word count. Uses a smoothing technique for words not present in the file.\n",
        "\n",
        "4. **<span style=\"color: blue;\">Word Segmentation with Context</span>**:\n",
        "   - The `wordSeqFitness` function:\n",
        "     - **Calculates Fitness Score**: Computes the fitness of a sequence of words based on their log probabilities using the `OneGramDist` class.\n",
        "\n",
        "5. **<span style=\"color: blue;\">Segmentation and Comparison</span>**:\n",
        "   - The `segment` function:\n",
        "     - **Segments Words**: Recursively segments a word into possible sequences and selects the one with the highest fitness score.\n",
        "   - The `segmentWithProb` function:\n",
        "     - **Calculates Segmentation and Probability**: Applies the `segment` function and returns both the segmentation and its fitness score for a given word.\n",
        "\n",
        "6. **<span style=\"color: blue;\">Example Usage</span>**:\n",
        "   - The code generates one-gram files for both medical and crime texts.\n",
        "   - It then uses these files to create `OneGramDist` instances and segments the word \"therapistfinder\" based on the text corpora to show how context affects segmentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "UDcFAkeY86gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "71kAFzsx9JEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here i am using two different corpuses for two different segmentation"
      ],
      "metadata": {
        "id": "3mmDmmMH89Eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "medical_txt='''Dr. Emily Thompson is a renowned therapist who specializes in finding innovative solutions for mental health issues. She has been helping patients with anxiety and depression for over two decades. Her practice, called \"Therapist Finder,\" aims to connect individuals with the right mental health professionals. Dr. Thompson's approach includes personalized therapy plans and advanced techniques to ensure the best outcomes for her patients. With a team of skilled therapists, the clinic offers various services to support mental well-being and overall health.\n",
        "'''"
      ],
      "metadata": {
        "id": "MJLMn-qwi1Rj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crime_txt= '''\n",
        "Detective John Carter is investigating a series of serious crimes in the city. The latest case involves a crime scene where a notorious criminal is suspected of committing a rape act. The \"Finder\" team, a specialized police unit, is working diligently to gather evidence and track down the rapist. The investigation includes interviewing witnesses, analyzing forensic evidence, and following leads to apprehend the rapist. Detective Carter is determined to solve the case and bring justice to the raped victims.\n",
        "'''"
      ],
      "metadata": {
        "id": "G3Y0Kvz9i9Ba"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now using the corpuse, here i am calculating the one-gram probabilities of each word."
      ],
      "metadata": {
        "id": "ilUd8iG39Ly2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "def preprocess_text(text):\n",
        "    # Convert to lower case\n",
        "    text = text.lower()\n",
        "    # Remove non-alphanumeric characters except for spaces\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def generate_one_gram_file(text, output_file):\n",
        "    # Preprocess the text\n",
        "    processed_text = preprocess_text(text)\n",
        "    # Tokenize the text\n",
        "    words = processed_text.split()\n",
        "    # Count the frequency of each word\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    # Write to output file\n",
        "    with open(output_file, 'w') as file:\n",
        "        for word, count in word_counts.items():\n",
        "            file.write(f\"{word}\\t{count}\\n\")\n"
      ],
      "metadata": {
        "id": "gtGPGxzvjUIp"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate one-gram file for the crime text\n",
        "generate_one_gram_file(crime_txt, 'crime-grams.txt')\n",
        "\n",
        "# Generate one-gram file for the medical text\n",
        "generate_one_gram_file(medical_txt, 'medical-grams.txt')"
      ],
      "metadata": {
        "id": "w967N0E1jV35"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Segmentation with Context:\n",
        "\n",
        "The wordSeqFitness function:\n",
        "Calculates Fitness Score: Computes the fitness of a sequence of words based on their log probabilities using the OneGramDist class.\n",
        "Segmentation and Comparison:\n",
        "\n",
        "## The segment function:\n",
        "Segments Words: Recursively segments a word into possible sequences and selects the one with the highest fitness score.\n",
        "The segmentWithProb function:\n",
        "Calculates Segmentation and Probability: Applies the segment function and returns both the segmentation and its fitness score for a given word."
      ],
      "metadata": {
        "id": "TJCIYel49Zlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools, math\n",
        "\n",
        "class OneGramDist(dict):\n",
        "   def __init__(self, filename):\n",
        "      self.gramCount = 0\n",
        "\n",
        "      for line in open(filename):\n",
        "         (word, count) = line[:-1].split('\\t')\n",
        "         self[word] = int(count)\n",
        "         self.gramCount += self[word]\n",
        "\n",
        "   def __call__(self, key):\n",
        "      if key in self:\n",
        "         return float(self[key]) / self.gramCount\n",
        "      else:\n",
        "         return 1.0 / (self.gramCount * 10**(len(key)-2))\n"
      ],
      "metadata": {
        "id": "Ggx-Vg3yKYN_"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "singleWordProb = OneGramDist('crime-grams.txt')\n",
        "def wordSeqFitness(words):\n",
        "   return sum(math.log10(singleWordProb(w)) for w in words)"
      ],
      "metadata": {
        "id": "EHSEzypiK9WE"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def memoize(f):\n",
        "   cache = {}\n",
        "\n",
        "   def memoizedFunction(*args):\n",
        "      if args not in cache:\n",
        "         cache[args] = f(*args)\n",
        "      return cache[args]\n",
        "\n",
        "   memoizedFunction.cache = cache\n",
        "   return memoizedFunction\n",
        "\n",
        "@memoize\n",
        "def segment(word):\n",
        "   if not word: return []\n",
        "   word = word.lower() # change to lower case\n",
        "   allSegmentations = [[first] + segment(rest) for (first,rest) in splitPairs(word)]\n",
        "   return max(allSegmentations, key = wordSeqFitness)\n",
        "\n",
        "def splitPairs(word, maxLen=20):\n",
        "   return [(word[:i+1], word[i+1:]) for i in range(max(len(word), maxLen))]\n",
        "\n",
        "@memoize\n",
        "def segmentWithProb(word):\n",
        "   segmented = segment(word)\n",
        "   return (wordSeqFitness(segmented), segmented)"
      ],
      "metadata": {
        "id": "DKTCjkDiaylo"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "word = \"therapistfinder\"\n",
        "score, segmented = segmentWithProb(word)\n",
        "print(\"Score:\", score)\n",
        "print(\"Segmentation:\", segmented)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rCU-GXMgKRZ",
        "outputId": "04fb5740-5b93-45a8-bab1-2302be0f397e"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: -4.488761291215399\n",
            "Segmentation: ['the', 'rapist', 'finder']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "singleWordProb2 = OneGramDist('medical-grams.txt')\n",
        "def wordSeqFitness(words):\n",
        "   return sum(math.log10(singleWordProb(w)) for w in words)\n"
      ],
      "metadata": {
        "id": "dQuzkMiTkNM7"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def memoize(f):\n",
        "   cache = {}\n",
        "\n",
        "   def memoizedFunction(*args):\n",
        "      if args not in cache:\n",
        "         cache[args] = f(*args)\n",
        "      return cache[args]\n",
        "\n",
        "   memoizedFunction.cache = cache\n",
        "   return memoizedFunction\n",
        "\n",
        "@memoize\n",
        "def segment(word):\n",
        "   if not word: return []\n",
        "   word = word.lower() # change to lower case\n",
        "   allSegmentations = [[first] + segment(rest) for (first,rest) in splitPairs(word)]\n",
        "   return max(allSegmentations, key = wordSeqFitness)\n",
        "\n",
        "def splitPairs(word, maxLen=20):\n",
        "   return [(word[:i+1], word[i+1:]) for i in range(max(len(word), maxLen))]\n",
        "\n",
        "@memoize\n",
        "def segmentWithProb(word):\n",
        "   segmented = segment(word)\n",
        "   return (wordSeqFitness(segmented), segmented)"
      ],
      "metadata": {
        "id": "PQqSDAVfkOZ_"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "word = \"therapistfinder\"\n",
        "score, segmented = segmentWithProb(word)\n",
        "print(\"Score:\", score)\n",
        "print(\"Segmentation:\", segmented)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxIZnEfekOdU",
        "outputId": "9cff851b-3c8d-4068-d3bc-2042284a8b6f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: -3.5159400420933182\n",
            "Segmentation: ['therapist', 'finder']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q4) Write a program that uses a bigram language model to predict the next word in a given context. The program should consider the previous word to predict the next word with the highest probability.**\n",
        "Task: Train a bigram model on a text corpus and predict the next word for a given input word.\n",
        "\n",
        "**Example:**\n",
        "Once upon a time there was a brave knight.\n",
        "\n",
        "The knight was known for his bravery and courage.\n",
        "\n",
        "He fought many battles and won the hearts of the people.\n",
        "\n",
        "The people of the kingdom loved the knight.\n",
        "\n",
        "One day, the knight embarked on a journey to defeat a dragon.\n",
        "\n",
        "The dragon was terrorizing the villages in the kingdom.\n",
        "\n",
        "After a long and fierce battle, the knight defeated the dragon.\n",
        "\n",
        "The people celebrated the victory of the knight.\n",
        "\n",
        "The knight became a legend in the kingdom.\n",
        "\n",
        "\n",
        "**what is most probable next word predicted by the modal?**\n",
        "\n",
        "\"The knight\"\n",
        "\n",
        "\"The people\"\n",
        "\n",
        "\"The dragon\"\n",
        "\n",
        "\"A brave\"\n",
        "\n",
        "\"The kingdom\"\n",
        "\n",
        "**Expected Predictions:**\n",
        "For \"The knight\", the model might predict \"was\".\n",
        "\n",
        "For \"The people\", it might predict \"of\" or \"celebrated\".\n",
        "\n",
        "For \"The dragon\", it could predict \"was\" or \"defeated\".\n",
        "\n",
        "For \"A brave\", it might predict \"knight\".\n",
        "\n",
        "For \"The kingdom\", it could predict \"loved\" or \"celebrated\"."
      ],
      "metadata": {
        "id": "Vk92vz3bVWeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = \"\"\"\n",
        "Once upon a time there was a brave knight.\n",
        "The knight was known for his bravery and courage.\n",
        "He fought many battles and won the hearts of the people.\n",
        "The people of the kingdom loved the knight.\n",
        "One day, the knight embarked on a journey to defeat a dragon.\n",
        "The dragon was terrorizing the villages in the kingdom.\n",
        "After a long and fierce battle, the knight defeated the dragon.\n",
        "The people celebrated the victory of the knight.\n",
        "The knight became a legend in the kingdom.\n",
        "\"\"\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lower case\n",
        "    # Remove punctuation and tokenize\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words\n",
        "\n",
        "def train_bigram_model(words):\n",
        "    bigram_counts = Counter()\n",
        "    unigram_counts = Counter(words)\n",
        "\n",
        "    for i in range(len(words) - 1):\n",
        "        bigram = (words[i], words[i+1])\n",
        "        bigram_counts[bigram] += 1\n",
        "\n",
        "    return bigram_counts, unigram_counts\n",
        "\n",
        "def calculate_probabilities(bigram_counts, unigram_counts):\n",
        "    probabilities = defaultdict(dict)\n",
        "\n",
        "    for (word1, word2), count in bigram_counts.items():\n",
        "        probabilities[word1][word2] = count / unigram_counts[word1]\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "def predict_next_word(context, probabilities):\n",
        "    context = context.lower()\n",
        "    words = context.split()\n",
        "    if len(words) == 0:\n",
        "        return None\n",
        "\n",
        "    previous_word = words[-1]\n",
        "\n",
        "    if previous_word not in probabilities:\n",
        "        return None\n",
        "\n",
        "    next_words = probabilities[previous_word]\n",
        "    predicted_word = max(next_words, key=next_words.get)\n",
        "    return predicted_word\n",
        "\n",
        "# Process corpus\n",
        "words = preprocess_text(corpus)\n",
        "bigram_counts, unigram_counts = train_bigram_model(words)\n",
        "probabilities = calculate_probabilities(bigram_counts, unigram_counts)\n",
        "\n",
        "# Predict next word\n",
        "contexts = [\n",
        "    \"The knight\",\n",
        "    \"The people\",\n",
        "    \"The dragon\",\n",
        "    \"A brave\",\n",
        "    \"The kingdom\"\n",
        "]\n",
        "\n",
        "for context in contexts:\n",
        "    predicted_word = predict_next_word(context, probabilities)\n",
        "    print(f\"Context: '{context}' -> Predicted next word: '{predicted_word}'\")\n"
      ],
      "metadata": {
        "id": "D4aJ3JtOcIu7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8a96dd-1928-4a99-8d85-20ce2fff053e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: 'The knight' -> Predicted next word: 'the'\n",
            "Context: 'The people' -> Predicted next word: 'the'\n",
            "Context: 'The dragon' -> Predicted next word: 'the'\n",
            "Context: 'A brave' -> Predicted next word: 'knight'\n",
            "Context: 'The kingdom' -> Predicted next word: 'loved'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## As in example we are generating third word from previous two words, so it is trigram, so below i have code a trigram language model."
      ],
      "metadata": {
        "id": "_NoFDwYA9iBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = \"\"\"\n",
        "Once upon a time there was a brave knight.\n",
        "The knight was known for his bravery and courage.\n",
        "He fought many battles and won the hearts of the people.\n",
        "The people of the kingdom loved the knight.\n",
        "One day, the knight embarked on a journey to defeat a dragon.\n",
        "The dragon was terrorizing the villages in the kingdom.\n",
        "After a long and fierce battle, the knight defeated the dragon.\n",
        "The people celebrated the victory of the knight.\n",
        "The knight became a legend in the kingdom.\n",
        "\"\"\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lower case\n",
        "    # Remove punctuation and tokenize\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words\n",
        "\n",
        "def train_trigram_model(words):\n",
        "    trigram_counts = Counter()\n",
        "    bigram_counts = Counter()\n",
        "    unigram_counts = Counter(words)\n",
        "\n",
        "    for i in range(len(words) - 2):\n",
        "        trigram = (words[i], words[i+1], words[i+2])\n",
        "        bigram = (words[i], words[i+1])\n",
        "        trigram_counts[trigram] += 1\n",
        "        bigram_counts[bigram] += 1\n",
        "\n",
        "    return trigram_counts, bigram_counts, unigram_counts\n",
        "\n",
        "def calculate_trigram_probabilities(trigram_counts, bigram_counts):\n",
        "    probabilities = defaultdict(dict)\n",
        "\n",
        "    for (word1, word2, word3), count in trigram_counts.items():\n",
        "        bigram = (word1, word2)\n",
        "        probabilities[bigram][word3] = count / bigram_counts[bigram]\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "def predict_next_word(context, probabilities):\n",
        "    context = context.lower()\n",
        "    words = context.split()\n",
        "\n",
        "    if len(words) < 2:\n",
        "        return None\n",
        "\n",
        "    bigram = (words[-2], words[-1])\n",
        "\n",
        "    if bigram not in probabilities:\n",
        "        return None\n",
        "\n",
        "    next_words = probabilities[bigram]\n",
        "    predicted_word = max(next_words, key=next_words.get)\n",
        "    return predicted_word\n",
        "\n",
        "# Process corpus\n",
        "words = preprocess_text(corpus)\n",
        "trigram_counts, bigram_counts, unigram_counts = train_trigram_model(words)\n",
        "probabilities = calculate_trigram_probabilities(trigram_counts, bigram_counts)\n",
        "\n",
        "# Predict next word\n",
        "contexts = [\n",
        "    \"The knight\",\n",
        "    \"The people\",\n",
        "    \"The dragon\",\n",
        "    \"A brave\",\n",
        "    \"The kingdom\"\n",
        "]\n",
        "\n",
        "for context in contexts:\n",
        "    predicted_word = predict_next_word(context, probabilities)\n",
        "    print(f\"Context: '{context}' -> Predicted next word: '{predicted_word}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwXOVnAoukqs",
        "outputId": "bfd14f88-c4a7-44f7-de5c-479889cbedbf"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: 'The knight' -> Predicted next word: 'was'\n",
            "Context: 'The people' -> Predicted next word: 'the'\n",
            "Context: 'The dragon' -> Predicted next word: 'was'\n",
            "Context: 'A brave' -> Predicted next word: 'knight'\n",
            "Context: 'The kingdom' -> Predicted next word: 'loved'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q5) Write a program that implements an N-gram (e.g., trigram) language model. The program should calculate the probability of a given sequence of words.**\n",
        "Task: Implement a trigram model using a provided text corpus. Calculate the probability of a given sentence.\n",
        "\n",
        "Corpus:\n",
        "\n",
        "I am a human.\n",
        "\n",
        "I am not a stone.\n",
        "\n",
        "I I live in Mumbai.\n",
        "\n",
        "Check the probability of \" I I am not \"\n",
        "\n"
      ],
      "metadata": {
        "id": "W8nNCRrScJKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = \"\"\"\n",
        "I am a human.\n",
        "I am not a stone.\n",
        "I I live in Mumbai.\n",
        "\"\"\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words\n",
        "\n",
        "def train_trigram_model(words):\n",
        "    trigram_counts = Counter()\n",
        "    bigram_counts = Counter()\n",
        "    unigram_counts = Counter(words)\n",
        "\n",
        "    for i in range(len(words) - 2):\n",
        "        trigram = (words[i], words[i+1], words[i+2])\n",
        "        bigram = (words[i], words[i+1])\n",
        "        trigram_counts[trigram] += 1\n",
        "        bigram_counts[bigram] += 1\n",
        "\n",
        "    return trigram_counts, bigram_counts, unigram_counts\n",
        "\n",
        "def calculate_trigram_probabilities(trigram_counts, bigram_counts):\n",
        "    probabilities = defaultdict(dict)\n",
        "\n",
        "    for (word1, word2, word3), count in trigram_counts.items():\n",
        "        bigram = (word1, word2)\n",
        "        probabilities[bigram][word3] = count / bigram_counts[bigram]\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "def calculate_sentence_probability(sentence, probabilities):\n",
        "    words = sentence.lower().split()\n",
        "    if len(words) < 3:\n",
        "        return 0.0\n",
        "\n",
        "    probability = 1.0\n",
        "    for i in range(len(words) - 2):\n",
        "        bigram = (words[i], words[i+1])\n",
        "        next_word = words[i+2]\n",
        "        if bigram in probabilities and next_word in probabilities[bigram]:\n",
        "            probability *= probabilities[bigram][next_word]\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    return probability\n"
      ],
      "metadata": {
        "id": "G-1JtgxHdXS6"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = preprocess_text(corpus)\n",
        "trigram_counts, bigram_counts, unigram_counts = train_trigram_model(words)\n",
        "probabilities = calculate_trigram_probabilities(trigram_counts, bigram_counts)\n",
        "\n",
        "sentence = \"I I am not\"\n",
        "probability = calculate_sentence_probability(sentence, probabilities)\n",
        "print(f\"Probability of the sentence '{sentence}': {probability}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63J5b4-5vfOr",
        "outputId": "2bd8bfda-6952-4c7f-95d8-fb44e2341358"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of the sentence 'I I am not': 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q6) Write a program that applies smoothing techniques (e.g., Laplace, Good-Turing) to a bigram language model. Compare the performance of the model with and without smoothing.**\n",
        "\n",
        "Corpus:\n",
        "\n",
        "I love natural language processing.\n",
        "\n",
        "Language models are great.\n",
        "\n",
        "I love machine learning.\n",
        "\n",
        "Machine learning is fun.\n",
        "\n",
        "Natural language processing is a complex field.\n"
      ],
      "metadata": {
        "id": "jVTsuDc_dXi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of Bigram Model with Smoothing Techniques\n",
        "\n",
        "1. **<span style=\"color: blue;\">Corpus and Preprocessing</span>**:\n",
        "   - The `corpus` contains several sentences used to train the bigram model.\n",
        "   - The `preprocess_text` function converts the text to lowercase and tokenizes it by removing punctuation. This prepares the text for further analysis.\n",
        "\n",
        "2. **<span style=\"color: blue;\">Training the Bigram Model</span>**:\n",
        "   - The `train_bigram_model` function calculates the counts of bigrams (pairs of consecutive words) and unigrams (individual words) from the tokenized text.\n",
        "   - It returns two counters: `bigram_counts` for bigram frequencies and `unigram_counts` for unigram frequencies.\n",
        "\n",
        "3. **<span style=\"color: blue;\">Laplace Smoothing</span>**:\n",
        "   - The `laplace_smoothing` function applies Laplace smoothing to handle zero probabilities for unseen bigrams.\n",
        "   - It adds 1 to each bigram count and divides by the sum of the unigram count plus the vocabulary size to ensure all bigrams have a non-zero probability.\n",
        "\n",
        "4. **<span style=\"color: blue;\">Good-Turing Smoothing</span>**:\n",
        "   - The `good_turing_smoothing` function applies Good-Turing smoothing, which adjusts the probabilities of observed bigrams based on the frequency of bigrams with similar counts.\n",
        "   - It uses the frequency of bigrams with counts `count + 1` to adjust the probability of bigrams with count `count`.\n",
        "\n",
        "5. **<span style=\"color: blue;\">Probability Calculation</span>**:\n",
        "   - The `calculate_probability` function computes the probability of a given sentence based on the provided bigram probabilities.\n",
        "   - It multiplies the probabilities of each bigram in the sentence. For unseen bigrams, it uses a default probability derived from the vocabulary size.\n",
        "\n",
        "6. **<span style=\"color: blue;\">Example Usage and Output</span>**:\n",
        "   - The code processes the `corpus`, trains the bigram model, and applies both Laplace and Good-Turing smoothing techniques.\n",
        "   - It calculates and prints the probability of the sentence `\"I love machine learning.\"` using unsmoothed, Laplace-smoothed, and Good-Turing-smoothed models.\n",
        "\n"
      ],
      "metadata": {
        "id": "A7C0u6PH-eSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramModel:\n",
        "    def __init__(self, corpus):\n",
        "        self.corpus = corpus\n",
        "        self.unigrams = Counter()\n",
        "        self.bigrams = defaultdict(Counter)\n",
        "        self.vocab_size = 0\n",
        "        self.total_bigrams = 0\n",
        "        self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        for sentence in self.corpus:\n",
        "            words = ['<s>'] + sentence.lower().split() + ['</s>']\n",
        "            self.unigrams.update(words)\n",
        "            for w1, w2 in zip(words, words[1:]):\n",
        "                self.bigrams[w1][w2] += 1\n",
        "                self.total_bigrams += 1\n",
        "        self.vocab_size = len(self.unigrams)\n",
        "\n",
        "    def probability(self, w1, w2):\n",
        "      if self.unigrams[w1] == 0:\n",
        "          return 1e-10  # Return a small value for unseen bigrams\n",
        "      return self.bigrams[w1][w2] / self.unigrams[w1]\n",
        "\n",
        "\n",
        "    def laplace_smoothing(self, w1, w2):\n",
        "        return (self.bigrams[w1][w2] + 1) / (self.unigrams[w1] + self.vocab_size)\n",
        "\n",
        "    def good_turing_smoothing(self, w1, w2):\n",
        "        count = self.bigrams[w1][w2]\n",
        "        if count == 0:\n",
        "            return 1 / self.total_bigrams\n",
        "        n_c = sum(1 for w in self.bigrams[w1].values() if w == count)\n",
        "        n_c_plus_1 = sum(1 for w in self.bigrams[w1].values() if w == count + 1)\n",
        "        if n_c_plus_1 == 0:\n",
        "            return count / self.total_bigrams\n",
        "        return ((count + 1) * n_c_plus_1 / n_c) / self.total_bigrams\n",
        "\n",
        "    def perplexity(self, test_sentence, smoothing=None):\n",
        "      words = ['<s>'] + test_sentence.lower().split() + ['</s>']\n",
        "      log_prob = 0\n",
        "      for w1, w2 in zip(words, words[1:]):\n",
        "          if smoothing == 'laplace':\n",
        "              prob = self.laplace_smoothing(w1, w2)\n",
        "          elif smoothing == 'good_turing':\n",
        "              prob = self.good_turing_smoothing(w1, w2)\n",
        "          else:\n",
        "              prob = self.probability(w1, w2)\n",
        "\n",
        "          # Ensure probability is never zero for log calculation\n",
        "          prob = max(prob, 1e-10)\n",
        "\n",
        "          log_prob += math.log2(prob)\n",
        "\n",
        "      return 2 ** (-log_prob / (len(words) - 1))\n"
      ],
      "metadata": {
        "id": "rCOhqv7t-WXJ"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Corpus\n",
        "corpus = [\n",
        "    \"I love natural language processing.\",\n",
        "    \"Language models are great.\",\n",
        "    \"I love machine learning.\",\n",
        "    \"Machine learning is fun.\",\n",
        "    \"Natural language processing is a complex field.\"\n",
        "]\n",
        "\n",
        "# Create and train the model\n",
        "model = BigramModel(corpus)\n",
        "\n",
        "# Test sentences\n",
        "test_sentences = [\n",
        "    \"I love language models.\",\n",
        "    \"Natural language processing is great.\",\n",
        "    \"Machine learning is complex.\",\n",
        "]\n",
        "\n",
        "# Compare perplexities\n",
        "for sentence in test_sentences:\n",
        "    print(f\"Test sentence: {sentence}\")\n",
        "    print(f\"No smoothing perplexity: {model.perplexity(sentence):.2f}\")\n",
        "    print(f\"Laplace smoothing perplexity: {model.perplexity(sentence, 'laplace'):.2f}\")\n",
        "    print(f\"Good-Turing smoothing perplexity: {model.perplexity(sentence, 'good_turing'):.2f}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3G2mwVh-4aH",
        "outputId": "1b7f4da6-a573-4458-ab99-f5bb88a670d9"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test sentence: I love language models.\n",
            "No smoothing perplexity: 1201124.43\n",
            "Laplace smoothing perplexity: 13.75\n",
            "Good-Turing smoothing perplexity: 21.98\n",
            "\n",
            "Test sentence: Natural language processing is great.\n",
            "No smoothing perplexity: 72.89\n",
            "Laplace smoothing perplexity: 11.17\n",
            "Good-Turing smoothing perplexity: 27.64\n",
            "\n",
            "Test sentence: Machine learning is complex.\n",
            "No smoothing perplexity: 15848.93\n",
            "Laplace smoothing perplexity: 13.81\n",
            "Good-Turing smoothing perplexity: 31.45\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q7) Write a program that implements a simple rule-based POS tagger. The program should take a sentence as input and assign POS tags based on predefined rules. For instance, you might define rules such as:**\n",
        "\n",
        "Words ending in \"ed\" are tagged as verbs (e.g., \"walked\" → VBD).\n",
        "\n",
        "Words ending in \"ly\" are tagged as adverbs (e.g., \"quickly\" → RB).\n",
        "Example:\n",
        "\n",
        "Input: \"The cat quickly jumped over the lazy dog.\"\n",
        "\n",
        "Output: [('The', 'DT'), ('cat', 'NN'), ('quickly', 'RB'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]"
      ],
      "metadata": {
        "id": "TJd9yLSVjGPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the `pos_tagging` Function\n",
        "\n",
        "1. **<span style=\"color: blue;\">Tagging Rules</span>**:\n",
        "   - The function uses predefined rules to assign part-of-speech (POS) tags to words in a sentence.\n",
        "   - **Determiners (DT)**: Tagged if the word is one of 'the', 'a', or 'an'.\n",
        "   - **Nouns (NN)**: Tagged if the word matches one of the predefined nouns (e.g., 'cat', 'dog', 'field', 'house').\n",
        "   - **Adverbs (RB)**: Tagged if the word ends with 'ly'.\n",
        "   - **Past Tense Verbs (VBD)**: Tagged if the word ends with 'ed'.\n",
        "   - **Prepositions (IN)**: Tagged if the word is one of 'over', 'under', 'in', 'on', or 'at'.\n",
        "   - **Adjectives (JJ)**: Tagged if the word matches one of the predefined adjectives (e.g., 'lazy', 'quick', 'happy').\n",
        "\n",
        "2. **<span style=\"color: blue;\">Tokenization</span>**:\n",
        "   - The input sentence is split into individual words for processing.\n",
        "\n",
        "3. **<span style=\"color: blue;\">Tagging Process</span>**:\n",
        "   - For each word, the function checks against the tagging rules:\n",
        "     - If the word matches any of the predefined lists or lambda functions, it is tagged accordingly.\n",
        "     - If the word does not match any rules, it is tagged as 'NN' (Noun) by default.\n",
        "\n",
        "4. **<span style=\"color: blue;\">Example Usage</span>**:\n",
        "   - Input Sentence: `\"The cat quickly jumped over the lazy dog.\"`\n",
        "   - The `pos_tagging` function is applied to the sentence, resulting in a list of tuples where each tuple contains a word and its POS tag.\n",
        "\n",
        "5. **<span style=\"color: blue;\">Output</span>**:\n",
        "   - The output is a list of tuples with each word from the input sentence paired with its corresponding POS tag.\n",
        "   - Example Output for the given sentence: `[('The', 'DT'), ('cat', 'NN'), ('quickly', 'RB'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]`\n",
        "\n"
      ],
      "metadata": {
        "id": "ljW4h5ye_RDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tagging(sentence):\n",
        "    # Define the rules for tagging\n",
        "    rules = {\n",
        "        'DT': ['the', 'a', 'an'],  # Determiners\n",
        "        'NN': lambda word: re.search(r'\\b(cat|dog|field|house)\\b', word),\n",
        "        'RB': lambda word: word.endswith('ly'),\n",
        "        'VBD': lambda word: word.endswith('ed'),\n",
        "        'IN': ['over', 'under', 'in', 'on', 'at'],\n",
        "        'JJ': lambda word: re.search(r'\\b(lazy|quick|happy)\\b', word),\n",
        "    }\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    words = sentence.split()\n",
        "\n",
        "    tagged_words = []\n",
        "    for word in words:\n",
        "        word_lower = word.lower()\n",
        "        tag_found = False\n",
        "\n",
        "        for tag, criteria in rules.items():\n",
        "            if isinstance(criteria, list):\n",
        "                if word_lower in criteria:\n",
        "                    tagged_words.append((word, tag))\n",
        "                    tag_found = True\n",
        "                    break\n",
        "            elif callable(criteria):\n",
        "                if criteria(word_lower):\n",
        "                    tagged_words.append((word, tag))\n",
        "                    tag_found = True\n",
        "                    break\n",
        "\n",
        "        if not tag_found:\n",
        "            tagged_words.append((word, 'NN'))\n",
        "\n",
        "    return tagged_words\n",
        "\n",
        "sentence = \"The cat quickly jumped over the lazy dog.\"\n",
        "tagged_sentence = pos_tagging(sentence)\n",
        "\n"
      ],
      "metadata": {
        "id": "iDgU-ygCjSl9"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_sentence"
      ],
      "metadata": {
        "id": "YMGHH9uXn-fx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54be65e8-ce64-4f6e-a945-328ec0be9a97"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('cat', 'NN'),\n",
              " ('quickly', 'RB'),\n",
              " ('jumped', 'VBD'),\n",
              " ('over', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('lazy', 'JJ'),\n",
              " ('dog.', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    }
  ]
}