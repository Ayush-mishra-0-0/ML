{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayush-mishra-0-0/ML/blob/main/ayush_12240340_assign2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<span script=\"color:cyan\">Ayush Kumar Mishra</span>  \n",
        "<span script=\"color:cyan\">12240340</span>  \n",
        "<span script=\"color:cyan\">Assignment-2</span>\n"
      ],
      "metadata": {
        "id": "T0rv6AXSziq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export CUDA_LAUNCH_BLOCKING=1"
      ],
      "metadata": {
        "id": "cKXkA1SeNNJr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Cloning the git repository AI4Bharat/IndicTrans2\n",
        "## And running all important libraries"
      ],
      "metadata": {
        "id": "hyDWurQ00F00"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qKcYlUZYGLrt"
      },
      "outputs": [],
      "source": [
        "# Clone the required Git repository for IndicTrans2\n",
        "%%capture\n",
        "!git clone https://github.com/AI4Bharat/IndicTrans2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U3vs7FkIGSxK"
      },
      "outputs": [],
      "source": [
        "# Clone the Hugging face interface from github\n",
        "%%capture\n",
        "%cd /content/IndicTrans2/huggingface_interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ddkRAXQ2Git0"
      },
      "outputs": [],
      "source": [
        "# Install other essential dependecies for working of the transformer\n",
        "%%capture\n",
        "!python3 -m pip install nltk sacremoses pandas regex mock transformers>=4.33.2 mosestokenizer\n",
        "!python3 -c \"import nltk; nltk.download('punkt')\"\n",
        "!python3 -m pip install bitsandbytes scipy accelerate datasets\n",
        "!python3 -m pip install sentencepiece\n",
        "\n",
        "!git clone https://github.com/VarunGumma/IndicTransTokenizer\n",
        "%cd IndicTransTokenizer\n",
        "!python3 -m pip install --editable ./\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing all the necessary libraries"
      ],
      "metadata": {
        "id": "ZnPu3-Mp0mXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from transformers.utils import is_flash_attn_2_available, is_flash_attn_greater_or_equal_2_10\n",
        "from IndicTransTokenizer import IndicProcessor\n",
        "from mosestokenizer import MosesSentenceSplitter\n",
        "from nltk import sent_tokenize\n",
        "from indicnlp.tokenize.sentence_tokenize import sentence_split, DELIM_PAT_NO_DANDA"
      ],
      "metadata": {
        "id": "dlBtwk5wgkfd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Although in the assignment we need only one translation, Though i am making checkpoints for all three directions possible\n",
        "\n",
        "1. ENGLISH TO INDIC\n",
        "2. INDIC TO ENGLISH\n",
        "3. INDIC TO INDIC"
      ],
      "metadata": {
        "id": "CAI_U-_r0s16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"  # this is checkpoint for english to indic translations\n",
        "indic_en_ckpt_dir = \"ai4bharat/indictrans2-indic-en-1B\"  #  this is checkpoint for indic to english translations\n",
        "indic_indic_ckpt_dir = (\n",
        "    \"ai4bharat/indictrans2-indic-indic-dist-320M\"  # this is checkpoint for indic to indic translations\n",
        ")"
      ],
      "metadata": {
        "id": "r6lndoAviuem"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenising the sentences\n",
        "---\n",
        "### Function: `split_sentences`\n",
        "\n",
        "This function, `split_sentences`, is designed to split sentences based on the language specified. Here's a breakdown of how it works:\n",
        "\n",
        "#### Parameters:\n",
        "- **`input_txt`**: The text that needs to be split into sentences.\n",
        "- **`lg`**: The language code in a short format. If the language is English, it's represented by `\"el\"`.\n",
        "\n",
        "#### Workflow:\n",
        "1. **English Language (`el`) Handling:**\n",
        "   - The function first tokenizes the input text using the `sent_tokenize` function and stores the sentences in the variable `s1`.\n",
        "   - It then uses the `MosesSentenceSplitter` (with the appropriate language code from `flores_codes`) to split the sentences further, storing the result in `s2`.\n",
        "   - Another sentence tokenization is performed on the input text, with results stored in `s3`.\n",
        "   - A comparison is made between `s3` and `s2`. If `s3` has fewer sentences, it's assigned to `s1`. Otherwise, `s2` is used.\n",
        "   - Any soft hyphen characters (`\"\\xad\"`) in the sentences are removed.\n",
        "\n",
        "2. **Other Languages:**\n",
        "   - For languages other than English, the `sentence_split` function is used to split the sentences. The language code from `flores_codes` and a delimiter pattern (`DELIM_PAT_NO_DANDA`) are provided as arguments.\n",
        "\n",
        "#### Return:\n",
        "- The function returns the split sentences in the form of a list, stored in the variable `s1`.\n",
        "\n",
        "#### Example:\n",
        "```python\n",
        "sentences = sp(\"This is an example text.\", \"el\")\n"
      ],
      "metadata": {
        "id": "lTietYvByBO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flores_codes = {\"asm_Beng\": \"as\", \"awa_Deva\": \"hi\", \"ben_Beng\": \"bn\", \"bho_Deva\": \"hi\", \"brx_Deva\": \"hi\", \"doi_Deva\": \"hi\", \"eng_Latn\": \"en\", \"gom_Deva\": \"kK\", \"guj_Gujr\": \"gu\", \"hin_Deva\": \"hi\", \"hne_Deva\": \"hi\", \"kan_Knda\": \"kn\", \"kas_Arab\": \"ur\", \"kas_Deva\": \"hi\", \"kha_Latn\": \"en\", \"lus_Latn\": \"en\", \"mag_Deva\": \"hi\", \"mai_Deva\": \"hi\", \"mal_Mlym\": \"ml\", \"mar_Deva\": \"mr\", \"mni_Beng\": \"bn\", \"mni_Mtei\": \"hi\", \"npi_Deva\": \"ne\", \"ory_Orya\": \"or\", \"pan_Guru\": \"pa\", \"san_Deva\": \"hi\", \"sat_Olck\": \"or\", \"snd_Arab\": \"ur\", \"snd_Deva\": \"hi\", \"tam_Taml\": \"ta\", \"tel_Telu\": \"te\", \"urd_Arab\": \"ur\"}\n"
      ],
      "metadata": {
        "id": "pHN8hpDaTwqE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_sentences(input_text, lang):\n",
        "    if lang == \"eng_Latn\":\n",
        "        input_sentences = sent_tokenize(input_text)\n",
        "        with MosesSentenceSplitter(flores_codes[lang]) as splitter:\n",
        "            sents_moses = splitter([input_text])\n",
        "        sents_nltk = sent_tokenize(input_text)\n",
        "        if len(sents_nltk) < len(sents_moses):\n",
        "            input_sentences = sents_nltk\n",
        "        else:\n",
        "            input_sentences = sents_moses\n",
        "        input_sentences = [sent.replace(\"\\xad\", \"\") for sent in input_sentences]\n",
        "    else:\n",
        "        input_sentences = sentence_split(\n",
        "            input_text, lang=flores_codes[lang], delim_pat=DELIM_PAT_NO_DANDA\n",
        "        )\n",
        "    return input_sentences\n"
      ],
      "metadata": {
        "id": "YMaYdlGbiwsJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now That we have tokens ready.. We have to make a inference of the `IndicTrans` model\n",
        "### Function: `initialize_model_and_tokenizer`\n",
        "\n",
        "This function, `initialize_model_and_tokenizer`, initializes a model and tokenizer based on specific configurations. Here's a breakdown of how it works:\n",
        "\n",
        "#### Parameters:\n",
        "- **`ck`**: The directory path to the model checkpoint.\n",
        "- **`qz`**: The quantization type, which can be either `\"4-bit\"` or `\"8-bit\"`.\n",
        "- **`ai`**: The attention implementation method. The options include `\"fa2\"` for Flash Attention 2 or `\"eg\"` for eager attention.\n",
        "\n",
        "#### Workflow:\n",
        "1. **Quantization Configuration:**\n",
        "   - If `qz` is `\"4-bit\"`, the `BitsAndBytesConfig` is initialized with 4-bit settings and stored in `qc`.\n",
        "   - If `qz` is `\"8-bit\"`, `BitsAndBytesConfig` is initialized with 8-bit settings and stored in `qc`.\n",
        "   - If neither, `qc` is set to `None`.\n",
        "\n",
        "2. **Attention Implementation:**\n",
        "   - If `ai` is `\"fa2\"` (Flash Attention 2), the function checks if Flash Attention 2 is available and compatible.\n",
        "   - If not, `ai` is set to `\"eg\"` (eager attention).\n",
        "\n",
        "3. **Model and Tokenizer Initialization:**\n",
        "   - The tokenizer is loaded using `AutoTokenizer` from the checkpoint specified by `ck`, with `trust_remote_code` enabled. The tokenizer is stored in `tk`.\n",
        "   - The model is loaded using `AutoModelForSeq2SeqLM`, with the attention implementation `ai`, quantization configuration `qc`, and low CPU memory usage enabled. The model is stored in `m`.\n",
        "   - If `qc` is `None`, the model is moved to the specified device (`DEVICE`), and half-precision floating point (`.half()`) is applied.\n",
        "\n",
        "4. **Model Evaluation Mode:**\n",
        "   - The model is set to evaluation mode using `m.eval()`.\n",
        "\n",
        "#### Return:\n",
        "- The function returns the initialized tokenizer (`tk`) and model (`m`).\n",
        "\n",
        "#### Example:\n",
        "```python\n",
        "tokenizer, model = im(\"path/to/checkpoint\", \"4-bit\", \"fa2\")\n"
      ],
      "metadata": {
        "id": "oVAXFXZ-4MAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model_and_tokenizer(ck, qz, ai):\n",
        "    if qz == \"4-bit\":\n",
        "        qc = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "    elif qz == \"8-bit\":\n",
        "        qc = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "            bnb_8bit_use_double_quant=True,\n",
        "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "    else:\n",
        "        qc = None\n",
        "\n",
        "    if ai == \"fa2\":\n",
        "        if is_flash_attn_2_available() and is_flash_attn_greater_or_equal_2_10():\n",
        "            ai = \"fa2\"\n",
        "        else:\n",
        "            ai = \"eg\"\n",
        "\n",
        "    tk = AutoTokenizer.from_pretrained(ck, trust_remote_code=True)\n",
        "    m = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        ck,\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=ai,\n",
        "        low_cpu_mem_usage=True,\n",
        "        quantization_config=qc,\n",
        "    )\n",
        "\n",
        "    if qc == None:\n",
        "        m = m.to(DEVICE)\n",
        "        m.half()\n",
        "\n",
        "    m.eval()\n",
        "\n",
        "    return tk, m\n"
      ],
      "metadata": {
        "id": "L12MleYSi0E0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔄 **Batch Translation Function**\n",
        "\n",
        "The `batch_translate` function performs translation of a batch of input sentences from a source language to a target language using a pre-trained model. Here's a step-by-step breakdown:\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Process Batches**\n",
        "- The function iterates over the input sentences in batches of size `BATCH_SIZE`. Each batch is processed individually to handle large datasets efficiently.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Preprocess Batch**\n",
        "- **Preprocessing**: Each batch is preprocessed by the `ip` (IndicProcessor) to prepare the data for tokenization and translation. This step involves tasks like normalizing text and handling entity mappings.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Tokenize Input**\n",
        "- **Tokenization**: The preprocessed batch is tokenized using the provided `tokenizer`. This converts the text into input encodings suitable for the model. The tokenization includes padding and truncation to handle variable-length sentences.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Generate Translations**\n",
        "- **Model Inference**: The model generates translations based on the tokenized inputs. This is done in a no-gradient context to save memory and computational resources. The `generate` method is used with parameters like `num_beams` for beam search, `min_length`, and `max_length` for controlling output length.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Decode Tokens**\n",
        "- **Decoding**: The generated tokens are decoded back into human-readable text using the tokenizer. Special tokens are removed, and the text is cleaned up to ensure proper formatting.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Postprocess Translations**\n",
        "- **Postprocessing**: The decoded translations are further processed by the `ip` to handle any necessary transformations, such as entity replacement or formatting adjustments.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Clean Up**\n",
        "- **Memory Management**: After processing each batch, the function clears up the memory by deleting intermediate variables and calling `torch.cuda.empty_cache()` to free GPU memory.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "The `batch_translate` function efficiently translates batches of sentences from a source language to a target language. It includes steps for preprocessing, tokenization, model inference, decoding, and postprocessing, with careful memory management throughout.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Process Overview**\n",
        "\n",
        "Below is a visual representation of the batch translation process:\n",
        "<!--\n",
        "![Batch Translation Process](https://drive.google.com/file/d/1gpu0Q-M_4Z7y7S0aobp2qFrNQdeNAiMY/view?usp=sharing) -->\n",
        "\n",
        "![Batch Translation Process](https://drive.google.com/uc?export=view&id=1gpu0Q-M_4Z7y7S0aobp2qFrNQdeNAiMY)\n",
        "\n"
      ],
      "metadata": {
        "id": "3FdqeEvoHUAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
        "    translations = []\n",
        "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
        "        batch = input_sentences[i : i + BATCH_SIZE]\n",
        "\n",
        "        # Preprocess the batch and extract entity mappings\n",
        "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "\n",
        "        # Tokenize the batch and generate input encodings\n",
        "        inputs = tokenizer(\n",
        "            batch,\n",
        "            truncation=True,\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=True,\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Generate translations using the model\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                use_cache=True,\n",
        "                min_length=0,\n",
        "                max_length=256,\n",
        "                num_beams=5,\n",
        "                num_return_sequences=1,\n",
        "            )\n",
        "\n",
        "        # Decode the generated tokens into text\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            generated_tokens = tokenizer.batch_decode(\n",
        "                generated_tokens.detach().cpu().tolist(),\n",
        "                skip_special_tokens=True,\n",
        "                clean_up_tokenization_spaces=True,\n",
        "            )\n",
        "\n",
        "        # Postprocess the translations, including entity replacement\n",
        "        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
        "\n",
        "        del inputs\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return translations\n",
        "\n"
      ],
      "metadata": {
        "id": "AxSOf35WEy2e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization of Models and Tokenizers\n",
        "\n",
        "In this section, we initialize the `IndicProcessor` for inference and set up three different models using the `im` function. Here’s the process:\n",
        "\n",
        "#### IndicProcessor Initialization:\n",
        "- **`ip`**: An instance of the `IndicProcessor` class is created with inference mode enabled.\n",
        "\n",
        "#### Models and Tokenizers Initialization:\n",
        "- **`t1, m1`**: The first pair represents the tokenizer and model for English to Indic translation. They are initialized using the `im` function with the checkpoint directory `en_indic_ckpt_dir`, quantization settings, and attention implementation specified.\n",
        "  \n",
        "- **`t2, m2`**: The second pair represents the tokenizer and model for Indic to English translation. These are also initialized using the `im` function, but with the `indic_en_ckpt_dir` directory.\n",
        "\n",
        "- **`t3, m3`**: The third pair is for Indic to Indic translation. They are initialized using the checkpoint directory `indic_indic_ckpt_dir`.\n",
        "\n",
        "#### Example:\n",
        "```python\n",
        "# Initialize the IndicProcessor\n",
        "ip = IndicProcessor(inference=True)\n",
        "\n",
        "# Initialize the models and tokenizers\n",
        "t1, m1 = im(en_indic_ckpt_dir, quantization, attn_implementation)\n",
        "t2, m2 = im(indic_en_ckpt_dir, quantization, attn_implementation)\n",
        "t3, m3 = im(indic_indic_ckpt_dir, quantization, attn_implementation)\n"
      ],
      "metadata": {
        "id": "yIrFotPCALEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I am using flash-attention to optimise the computation of attention."
      ],
      "metadata": {
        "id": "zU7Omt8YEX3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip = IndicProcessor(inference=True)\n",
        "quantization = \"4-bit\"\n",
        "attn_implementation = \"fa2\"\n",
        "\n",
        "en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(\n",
        "    en_indic_ckpt_dir, quantization, attn_implementation\n",
        ")\n",
        "\n",
        "indic_en_tokenizer, indic_en_model = initialize_model_and_tokenizer(\n",
        "    indic_en_ckpt_dir, quantization, attn_implementation\n",
        ")\n",
        "\n",
        "indic_indic_tokenizer, indic_indic_model = initialize_model_and_tokenizer(\n",
        "    indic_indic_ckpt_dir, quantization, attn_implementation\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jptUuJli6EU",
        "outputId": "d2863dd0-5839-4dbd-d63b-d8c3f6f1ad98"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "hi_sents = [\n",
        "    \"जब मैं छोटा था, मैं हर रोज़ पार्क जाता था।\",\n",
        "    \"उसके पास बहुत सारी पुरानी किताबें हैं, जिन्हें उसने अपने दादा-परदादा से विरासत में पाया।\",\n",
        "    \"मुझे समझ में नहीं आ रहा कि मैं अपनी समस्या का समाधान कैसे ढूंढूं।\",\n",
        "    \"वह बहुत मेहनती और समझदार है, इसलिए उसे सभी अच्छे मार्क्स मिले।\",\n",
        "    \"हमने पिछले सप्ताह एक नई फिल्म देखी जो कि बहुत प्रेरणादायक थी।\",\n",
        "    \"अगर तुम मुझे उस समय पास मिलते, तो हम बाहर खाना खाने चलते।\",\n",
        "    \"वह अपनी दीदी के साथ बाजार गयी थी ताकि वह नई साड़ी खरीद सके।\",\n",
        "    \"राज ने मुझसे कहा कि वह अगले महीने अपनी नानी के घर जा रहा है।\",\n",
        "    \"सभी बच्चे पार्टी में मज़ा कर रहे थे और खूब सारी मिठाइयाँ खा रहे थे।\",\n",
        "    \"मेरे मित्र ने मुझे उसके जन्मदिन की पार्टी में बुलाया है, और मैं उसे एक तोहफा दूंगा।\",\n",
        "]\n",
        "src_lang, tgt_lang = \"hin_Deva\", \"mar_Deva\"\n",
        "mr_translations = batch_translate(\n",
        "    hi_sents, src_lang, tgt_lang, indic_indic_model, indic_indic_tokenizer, ip\n",
        ")\n",
        "\n",
        "print(f\"\\n{src_lang} - {tgt_lang}\")\n",
        "for input_sentence, translation in zip(hi_sents, mr_translations):\n",
        "    print(f\"{src_lang}: {input_sentence}\")\n",
        "    print(f\"{tgt_lang}: {translation}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiIR_-OokAgc",
        "outputId": "b2a5f67f-02ab-4a7e-d7c1-50bd5feebd1f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hin_Deva - mar_Deva\n",
            "hin_Deva: जब मैं छोटा था, मैं हर रोज़ पार्क जाता था।\n",
            "mar_Deva: जेव्हा मी लहान होतो, तेव्हा मी दररोज उद्यानात जायचे. \n",
            "hin_Deva: उसके पास बहुत सारी पुरानी किताबें हैं, जिन्हें उसने अपने दादा-परदादा से विरासत में पाया।\n",
            "mar_Deva: तिच्याकडे बरेच जुने पुस्तक आहे, जे तिला तिच्या आजी-आजोबांकडून वारसा मिळाले. \n",
            "hin_Deva: मुझे समझ में नहीं आ रहा कि मैं अपनी समस्या का समाधान कैसे ढूंढूं।\n",
            "mar_Deva: माझ्या समस्येचे निराकरण कसे करावे हे मला समजत नाही. \n",
            "hin_Deva: वह बहुत मेहनती और समझदार है, इसलिए उसे सभी अच्छे मार्क्स मिले।\n",
            "mar_Deva: तो खूप मेहनती आणि समजदार आहे, म्हणून त्याला सर्व चांगले गुण मिळाले. \n",
            "hin_Deva: हमने पिछले सप्ताह एक नई फिल्म देखी जो कि बहुत प्रेरणादायक थी।\n",
            "mar_Deva: आम्ही गेल्या आठवड्यात एक नवीन चित्रपट पाहिला जो खूप प्रेरणादायी होता. \n",
            "hin_Deva: अगर तुम मुझे उस समय पास मिलते, तो हम बाहर खाना खाने चलते।\n",
            "mar_Deva: जर तुम्हाला त्या वेळी माझ्याकडे भेट दिली असती, तर आम्ही बाहेर जेवायला गेलो होतो. \n",
            "hin_Deva: वह अपनी दीदी के साथ बाजार गयी थी ताकि वह नई साड़ी खरीद सके।\n",
            "mar_Deva: ती तिच्या दीदीसोबत बाजारात गेली जेणेकरून ती नवीन साडी खरेदी करू शकेल. \n",
            "hin_Deva: राज ने मुझसे कहा कि वह अगले महीने अपनी नानी के घर जा रहा है।\n",
            "mar_Deva: राजने मला सांगितले की तो पुढच्या महिन्यात त्याच्या आजीच्या घरी जात आहे. \n",
            "hin_Deva: सभी बच्चे पार्टी में मज़ा कर रहे थे और खूब सारी मिठाइयाँ खा रहे थे।\n",
            "mar_Deva: सर्व मुले पार्टीत मजा करत होती आणि भरपूर गोड पदार्थ खात होती. \n",
            "hin_Deva: मेरे मित्र ने मुझे उसके जन्मदिन की पार्टी में बुलाया है, और मैं उसे एक तोहफा दूंगा।\n",
            "mar_Deva: माझ्या मित्राने मला तिच्या वाढदिवसाच्या पार्टीत बोलावले आहे आणि मी तिला एक भेट देईन. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation metrics\n",
        "\n",
        "## For evaluation how this model is performing, we must have to original translations on which we can check whether this model is close to this or not.\n",
        "\n",
        "## For this purpose i am using ` FLORES-22 Indic dev set` dataset which is english - indic translations"
      ],
      "metadata": {
        "id": "N4lqyaew4NjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First i am mounting the google drive in which dataset is present."
      ],
      "metadata": {
        "id": "DJyw-Uk5BJJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiVXJz03Aw7E",
        "outputId": "e9729b03-72d6-4763-ba8c-dbdb12a1d684"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu rouge-score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nRpmWdXBkfX",
        "outputId": "a231384c-bfb6-41a8-c14d-d54089c2ac94"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.3)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.10.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.5.15)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For showcase, I am using english-bengali dataset from the `FLORES-22 Indic dev set`"
      ],
      "metadata": {
        "id": "goY4kMwPFaHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📝 **Translation Process from English to Bengali**\n",
        "\n",
        "In this translation process, we convert English sentences to Bengali using a pre-trained model. Below is a breakdown of the steps:\n",
        "\n",
        "---\n",
        "\n",
        "#### 📂 **Step 1: Loading Sentences**\n",
        "- **English Sentences**: The English text is loaded from a file. Each line in the file corresponds to a different English sentence.\n",
        "- **Bengali Sentences**: The Bengali text is loaded from another file. Each line represents the corresponding Bengali sentence.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🌐 **Step 2: Setting Language Codes**\n",
        "- **Source Language**: The language code for English is specified as `eng_Latn`, which denotes English in Latin script.\n",
        "- **Target Language**: The language code for Bengali is set to `ben_beng`, representing Bengali in Bengali script.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔄 **Step 3: Translation Using Pre-trained Model**\n",
        "- The English sentences are passed to a pre-trained model designed for translation. This model translates English sentences into Bengali.\n",
        "\n",
        "---\n",
        "\n",
        "#### 📊 **Step 4: Printing Results**\n",
        "- The translated Bengali sentences are then displayed alongside the original English sentences for comparison.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "JmcxkkoMGczU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eng_path = \"/content/drive/MyDrive/flores-22_dev/flores-22_dev/all/eng_Latn-asm_Beng/dev.eng_Latn\"\n",
        "beng_path = \"/content/drive/MyDrive/flores-22_dev/flores-22_dev/all/eng_Latn-asm_Beng/dev.asm_Beng\"\n",
        "# /content/drive/MyDrive/flores-22_dev/flores-22_dev/all/eng_Latn-asm_Beng/dev.asm_Beng\n",
        "def get_first_n_lines(sentences, n):\n",
        "    \"\"\"Returns the first n lines as a single string.\"\"\"\n",
        "    return ' '.join(sentence.strip() for sentence in sentences[:n])\n",
        "\n",
        "# Define the number of lines to include in the paragraph\n",
        "num_lines = 10\n",
        "\n",
        "# Load the English sentences\n",
        "with open(eng_path, 'r', encoding='utf-8') as f:\n",
        "    eng_sentences = f.readlines()\n",
        "\n",
        "# Load the Bengali sentences\n",
        "with open(beng_path, 'r', encoding='utf-8') as f:\n",
        "    beng_sentences = f.readlines()\n",
        "\n",
        "# Get the first 10 lines for both English and Bengali\n",
        "eng_paragraph = get_first_n_lines(eng_sentences, num_lines)\n",
        "beng_paragraph = get_first_n_lines(beng_sentences, num_lines)\n",
        "\n",
        "# Print the paragraphs\n",
        "print(\"First 10 lines of English sentences as a paragraph:\")\n",
        "print(eng_paragraph)\n",
        "\n",
        "print(\"\\nFirst 10 lines of Bengali sentences as a paragraph:\")\n",
        "print(beng_paragraph)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHxk2igYjGUp",
        "outputId": "730fcab4-7866-4e35-e993-c55bc0f1319f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 lines of English sentences as a paragraph:\n",
            "On Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each. Lead researchers say this may bring early detection of cancer, tuberculosis, HIV and malaria to patients in low-income countries, where the survival rates for illnesses such as breast cancer can be half those of richer countries. The JAS 39C Gripen crashed onto a runway at around 9:30 am local time (0230 UTC) and exploded, closing the airport to commercial flights. The pilot was identified as Squadron Leader Dilokrit Pattavee. Local media reports an airport fire vehicle rolled over while responding. 28-year-old Vidal had joined Barça three seasons ago, from Sevilla. Since moving to the Catalan-capital, Vidal had played 49 games for the club. The protest started around 11:00 local time (UTC+1) on Whitehall opposite the police-guarded entrance to Downing Street, the Prime Minister's official residence. Just after 11:00, protesters blocked traffic on the northbound carriage in Whitehall. At 11:20, the police asked the protesters to move back on to the pavement, stating that they needed to balance the right to protest with the traffic building up.\n",
            "\n",
            "First 10 lines of Bengali sentences as a paragraph:\n",
            "সোমবাৰে, ষ্টেনফ'ৰ্ড ইউনিভাৰচিটি স্কুল অৱ মেডিচিনৰ বিজ্ঞানীসকলে  ঘোষণা কৰিছিল যে তেওঁলোকে এটা নতুন ৰোগ চিনাক্তকৰণ সঁজুলি আৱিষ্কাৰ কৰিছিল যিয়ে ধৰণ অনুসৰি কোষসমূহ ক্ৰম কৰে: এটা ক্ষুদ্ৰ মুদ্ৰণযোগ্য চিপ যিটো প্ৰত্যেকটোতে সম্ভৱপৰ প্ৰায় 1 U.S. চেণ্ট খৰচ হোৱাকৈ ষ্টেণ্ডাৰ্ড ইনজেক্ট প্ৰিণ্টাৰ ব্যৱহাৰ কৰি প্ৰস্তুত কৰা হৈছে। শীৰ্ষ গৱেষকে কয় যে এইটোৱে কেন্সাৰ, যক্ষ্মা, এইচ.আই.ভি আৰু মেলেৰিয়াৰ প্ৰাথমিক পৰ্য্যায়ৰ চিনাক্তকৰণ নিম্ন আয়ৰ দেশবিলাকৰ ৰোগীসকললৈ আনিব পাৰে , য’ত স্তন কেন্সাৰৰ দৰে বেমাৰবোৰৰ সুস্থ হোৱাৰ হাৰ ধনী দেশৰ তুলনাত অৰ্ধেক হ’ব পাৰে। স্থানীয় সময় (0230 UTC) অনুসৰি পুৱা 9:30 বজাত ৰানৱেত খুন্দা মাৰি দুর্ঘটনাত পতিত হোৱাৰ ফলত JAS 39C গ্রিপেন বিস্ফোৰিত হৈছিল যাৰ ফলত সেই বিমানঘাটিলৈ সমূহ বাণিজ্যিক বিমানৰ উৰণ বন্ধ হৈ পৰিছিল। পাইলটগৰাকী স্কুৱাড্ৰন লীডাৰ ডিলক্ৰিট পাটাভি বুলি চিনাক্ত কৰা হৈছে। স্থানীয় সংবাদ মাধ্যমে সঁহাৰি জনাওতে বিমানবন্দৰৰ এখন অগ্নি নিৰ্বাপক বাহন বাগৰি পৰাৰ বৃত্তান্ত দিছিল। ছেভিলাৰ 28 বছৰীয়া ভিডেলে 3 বছৰৰ আগতে অনুষ্ঠিত হোৱা বার্ছাত যোগদান কৰিছিল। কেটালানৰ ৰাজধানীলৈ যোৱাৰ পাছত ভিডালে ক্লাবটোৰ হৈ ৪৯ খন খেল খেলিছে। প্রধান মন্ত্রীৰ চৰকাৰী বাসভৱন ডাওনিং ষ্ট্রিটৰ আৰক্ষীৰ বেষ্টনীৰ মাজত থকা প্রৱেশদ্বাৰৰ বিপৰীত দিশে থকা হোৱাইট হলত স্থানীয় সময় অনুসৰি 11:00 বজাত (UTC+1) প্রতিবাদী কার্যসূচী আৰম্ভ হয়। ঠিক 11:00ৰ পিছত প্রতিবাদকাৰীসকলে হোৱাইট হলৰ নর্থবাউণ্ড কেৰেজৰ ট্রেফিক অৱৰোধ কৰে। 11:20 বজাত পুলিচে প্রতিবাদকাৰীসকলক এইবুলি কৈ পদপথলৈ ঘূৰাই পঠাইছিল যে তেওঁলোকে প্রতিবাদৰ অধিকাৰ আৰু ট্রেফিকৰ মাজত সন্তুলন বজাই ৰাখিব লাগিব।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_paragraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "lS-kOhq1R6Hd",
        "outputId": "120cb16b-64e2-4c58-a3cc-0df8098d77b7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"On Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each. Lead researchers say this may bring early detection of cancer, tuberculosis, HIV and malaria to patients in low-income countries, where the survival rates for illnesses such as breast cancer can be half those of richer countries. The JAS 39C Gripen crashed onto a runway at around 9:30 am local time (0230 UTC) and exploded, closing the airport to commercial flights. The pilot was identified as Squadron Leader Dilokrit Pattavee. Local media reports an airport fire vehicle rolled over while responding. 28-year-old Vidal had joined Barça three seasons ago, from Sevilla. Since moving to the Catalan-capital, Vidal had played 49 games for the club. The protest started around 11:00 local time (UTC+1) on Whitehall opposite the police-guarded entrance to Downing Street, the Prime Minister's official residence. Just after 11:00, protesters blocked traffic on the northbound carriage in Whitehall. At 11:20, the police asked the protesters to move back on to the pavement, stating that they needed to balance the right to protest with the traffic building up.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_paragraph(input_text, src_lang, tgt_lang, model, tokenizer, ip):\n",
        "    input_sentences = split_sentences(input_text, src_lang)\n",
        "    translated_text = batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip)\n",
        "    return \" \".join(translated_text)"
      ],
      "metadata": {
        "id": "IabyFxAOMTyX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_paragraph(\n",
        "    \"On Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each. Lead researchers say this may bring early detection of cancer, tuberculosis, HIV and malaria to patients in low-income countries, where the survival rates for illnesses such as breast cancer can be half those of richer countries. The JAS 39C Gripen crashed onto a runway at around 9:30 am local time (0230 UTC) and exploded, closing the airport to commercial flights. The pilot was identified as Squadron Leader Dilokrit Pattavee. Local media reports an airport fire vehicle rolled over while responding. 28-year-old Vidal had joined Barça three seasons ago, from Sevilla. Since moving to the Catalan-capital, Vidal had played 49 games for the club. The protest started around 11:00 local time (UTC+1) on Whitehall opposite the police-\"\n",
        "    , src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "YFB8y0-jR_rT",
        "outputId": "023e88de-36f8-4a13-ff8b-02e7a87e862f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'सोमवार को, स्टैनफोर्ड यूनिवर्सिटी स्कूल ऑफ मेडिसिन के वैज्ञानिकों ने एक नए नैदानिक उपकरण के आविष्कार की घोषणा की जो कोशिकाओं को प्रकार के अनुसार क्रमबद्ध कर सकता हैः एक छोटी छापने योग्य चिप जिसे संभवतः लगभग एक यू. एस. के लिए मानक इंकजेट प्रिंटर का उपयोग करके बनाया जा सकता है । प्रत्येक प्रतिशत । प्रमुख शोधकर्ताओं का कहना है कि इससे कम आय वाले देशों में कैंसर, तपेदिक, एच. आई. वी. और मलेरिया का जल्दी पता चल सकता है, जहां स्तन कैंसर जैसी बीमारियों के लिए जीवित रहने की दर अमीर देशों की तुलना में आधी हो सकती है । जे. ए. एस. 39सी. ग्रिपेन स्थानीय समयानुसार सुबह लगभग 9:30 बजे (0230 यू. टी. सी.) एक रनवे पर दुर्घटनाग्रस्त हो गया और विस्फोट हो गया, जिससे हवाई अड्डे को वाणिज्यिक उड़ानों के लिए बंद कर दिया गया । पायलट की पहचान स्क्वाड्रन लीडर दिलोक्रित पट्टवी के रूप में की गई थी । स्थानीय मीडिया ने बताया कि हवाई अड्डे पर आग बुझाने वाला एक वाहन जवाबी कार्रवाई करते हुए पलट गया । 28 वर्षीय विडाल तीन सत्र पहले सेविला से बार्सिलोना में शामिल हुआ था । कैटलन - राजधानी में जाने के बाद से, विडाल ने क्लब के लिए 49 मैच खेले थे । विरोध प्रदर्शन स्थानीय समय (यूटीसी + 1) के आसपास व्हाइटहॉल में पुलिस के सामने शुरू हुआ -'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "OmZWK67bMGxn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
      ],
      "metadata": {
        "id": "a3NZd7mzMRR5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "src_lang, tgt_lang = \"eng_Latn\", \"ben_beng\"\n",
        "translate_paragraph(\n",
        "    '''On Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each.\n",
        "Lead researchers say this may bring early detection of cancer, tuberculosis, HIV and malaria to patients in low-income countries, where the survival rates for illnesses such as breast cancer can be half those of richer countries.\n",
        "The JAS 39C Gripen crashed onto a runway at around 9:30 am local time (0230 UTC) and exploded, closing the airport to commercial flights.\n",
        "The pilot was identified as Squadron Leader Dilokrit Pattavee.\n",
        "Local media reports an airport fire vehicle rolled over while responding.\n",
        "28-year-old Vidal had joined Barça three seasons ago, from Sevilla.\n",
        "Since moving to the Catalan-capital, Vidal had played 49 games for the club.\n",
        "The protest started around 11:00 local time (UTC+1) on Whitehall opposite the police-guarded entrance to Downing Street, the Prime Minister's official residence.\n",
        "''', src_lang, tgt_lang, en_indic_model, indic_indic_tokenizer, ip\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "QCZB-qjRQJHl",
        "outputId": "60389121-37d7-49c3-b113-09fc6477c17f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-94886f0b0015>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"eng_Latn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ben_beng\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m translate_paragraph(\n\u001b[0m\u001b[1;32m      3\u001b[0m     '''On Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each.\n\u001b[1;32m      4\u001b[0m \u001b[0mLead\u001b[0m \u001b[0mresearchers\u001b[0m \u001b[0msay\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mbring\u001b[0m \u001b[0mearly\u001b[0m \u001b[0mdetection\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcancer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuberculosis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIV\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmalaria\u001b[0m \u001b[0mto\u001b[0m \u001b[0mpatients\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mincome\u001b[0m \u001b[0mcountries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msurvival\u001b[0m \u001b[0mrates\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0millnesses\u001b[0m \u001b[0msuch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbreast\u001b[0m \u001b[0mcancer\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mhalf\u001b[0m \u001b[0mthose\u001b[0m \u001b[0mof\u001b[0m \u001b[0mricher\u001b[0m \u001b[0mcountries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mJAS\u001b[0m \u001b[0;36m39\u001b[0m\u001b[0mC\u001b[0m \u001b[0mGripen\u001b[0m \u001b[0mcrashed\u001b[0m \u001b[0monto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mrunway\u001b[0m \u001b[0mat\u001b[0m \u001b[0maround\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m \u001b[0mam\u001b[0m \u001b[0mlocal\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;36m230\u001b[0m \u001b[0mUTC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexploded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mairport\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcommercial\u001b[0m \u001b[0mflights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-9decc4b313f8>\u001b[0m in \u001b[0;36mtranslate_paragraph\u001b[0;34m(input_text, src_lang, tgt_lang, model, tokenizer, ip)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_paragraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtranslated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-5eceddeae583>\u001b[0m in \u001b[0;36mbatch_translate\u001b[0;34m(input_sentences, src_lang, tgt_lang, model, tokenizer, ip)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         ).to(DEVICE)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Generate translations using the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;31m# into a HalfTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;31m# into a HalfTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_lang, tgt_lang = \"eng_Latn\", \"ben_beng\"\n",
        "mr_translation = translate_paragraph(\n",
        "    eng_paragraph, src_lang, tgt_lang, en_indic_model, indic_indic_tokenizer, ip\n",
        ")\n",
        "\n",
        "print(f\"\\n{src_lang} - {tgt_lang}\")\n",
        "for input_sentence, translation in zip(eng_sentences, mr_translation):\n",
        "    print(f\"{src_lang}: {input_sentence}\")\n",
        "    print(f\"{tgt_lang}: {translation}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "ayDpT3S2Cf9U",
        "outputId": "f13170c2-ba38-4be0-9600-1592797b595e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-482c8f0a7a9d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"eng_Latn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ben_beng\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m mr_translation = translate_paragraph(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0meng_paragraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_indic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindic_indic_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-9decc4b313f8>\u001b[0m in \u001b[0;36mtranslate_paragraph\u001b[0;34m(input_text, src_lang, tgt_lang, model, tokenizer, ip)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_paragraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtranslated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-5eceddeae583>\u001b[0m in \u001b[0;36mbatch_translate\u001b[0;34m(input_sentences, src_lang, tgt_lang, model, tokenizer, ip)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         ).to(DEVICE)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Generate translations using the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;31m# into a HalfTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;31m# into a HalfTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📝 **Evaluating Translation Quality: BLEU and ROUGE Scores**\n",
        "\n",
        "This evaluation process measures the quality of translations using BLEU and ROUGE metrics. Below is a breakdown of the steps involved:\n",
        "\n",
        "---\n",
        "\n",
        "#### 📂 **Step 1: Import Libraries**\n",
        "- **BLEU Calculation**: Import `corpus_bleu` from the `nltk.translate.bleu_score` library for computing the BLEU score.\n",
        "- **ROUGE Calculation**: Import `RougeScorer` from the `rouge_score` library for calculating ROUGE scores.\n",
        "\n",
        "---\n",
        "\n",
        "#### 📄 **Step 2: Load and Preprocess Data**\n",
        "- **Reference Sentences**: Load the reference Bengali sentences from a file and split them into tokens (words).\n",
        "- **Machine-Generated Translations**: Similarly, process the machine-generated Bengali translations into tokens.\n",
        "\n",
        "---\n",
        "\n",
        "#### 📊 **Step 3: Calculate BLEU Score**\n",
        "- **BLEU Score**: Use the `corpus_bleu` function to compute the BLEU score, which measures how closely the machine-generated translations match the reference sentences in terms of n-gram precision.\n",
        "\n",
        "#### 📈 **Step 4: Calculate ROUGE Scores**\n",
        "ROUGE Scorer: Initialize the RougeScorer with ROUGE metrics (rouge1, rouge2, rougeL) to compute ROUGE scores, which evaluate the overlap of n-grams and sequences between the generated and reference translations."
      ],
      "metadata": {
        "id": "fwbMOiMQLS7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = sacrebleu.corpus_bleu(beng_sentences, [mr_references])\n",
        "print(f\"BLEU score: {bleu.score}\")"
      ],
      "metadata": {
        "id": "0_tp1-6KBo0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Assuming you have reference Bengali sentences loaded\n",
        "with open(beng_path, 'r', encoding='utf-8') as f:\n",
        "    reference_sentences = f.readlines()\n",
        "\n",
        "# Preprocess reference sentences to match BLEU/ROUGE format\n",
        "reference_sentences = [sent.split() for sent in reference_sentences]\n",
        "mr_translations = [sent.split() for sent in mr_translations]\n",
        "\n",
        "# Compute BLEU score\n",
        "bleu_score = corpus_bleu([reference_sentences], mr_translations)\n",
        "print(f\"BLEU score: {bleu_score}\")\n",
        "\n",
        "# Compute ROUGE score\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
        "for ref, pred in zip(reference_sentences, mr_translations):\n",
        "    scores = scorer.score(' '.join(ref), ' '.join(pred))\n",
        "    for key in rouge_scores:\n",
        "        rouge_scores[key] += scores[key].fmeasure\n",
        "\n",
        "# Average ROUGE scores\n",
        "num_sentences = len(reference_sentences)\n",
        "rouge_scores = {key: value / num_sentences for key, value in rouge_scores.items()}\n",
        "print(f\"ROUGE scores: {rouge_scores}\")\n"
      ],
      "metadata": {
        "id": "VefKe10UKtZ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}