{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayush-mishra-0-0/ML/blob/main/ayushKumarMishra_12240340_temp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <h1 align=\"center\"> DS504: NLP - Lab</h1>\n",
        "<h1 align=\"center\">Assignment - 2</h1>\n",
        "<h1 align=\"center\">27/08/2024</h1>\n",
        "\n",
        "---\n",
        "<font color=\"lightgreen\">**Name:** Ayush Kumar Mishra <br>\n",
        "**Roll No:** 12240340 </font>"
      ],
      "metadata": {
        "id": "_Grt4MzOZ4_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Questions**\n"
      ],
      "metadata": {
        "id": "KAHG_psOk2pb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q1) Write a program that parses a given word into its root form and affixes (prefixes and suffixes). The program should output the morphemes.**\n",
        "Example Input: \"unhappiness\"\n",
        "\n",
        "Example Output: [\"un-\", \"happy\", \"-ness\"]"
      ],
      "metadata": {
        "id": "FgF-CUm1le5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the `solve` Function\n",
        "\n",
        "1. **<span style=\"color: blue;\">Prefix and Suffix Lists</span>**:\n",
        "   - The function defines two lists: `prefixes` and `suffixes`.\n",
        "   - <span style=\"color: green;\">**Prefixes**</span>: `['un', 're', 'in', 'dis', 'im', 'ir', 'il']`\n",
        "   - <span style=\"color: green;\">**Suffixes**</span>: `['ed', 'ing', 'ly', 'ness', 's', 'er', 'able', 'ment']`\n",
        "   - These lists contain common prefixes and suffixes used for word transformation.\n",
        "\n",
        "2. **<span style=\"color: blue;\">Prefix Removal</span>**:\n",
        "   - The function checks if the input `word` starts with any of the prefixes from the `prefixes` list.\n",
        "   - If a match is found, the prefix is removed from the word, and the prefix is printed.\n",
        "\n",
        "3. **<span style=\"color: blue;\">Suffix Removal</span>**:\n",
        "   - After removing a prefix (if any), the function checks if the remaining base word ends with any of the suffixes from the `suffixes` list.\n",
        "   - If a match is found, the suffix is removed from the word, and the suffix is printed.\n",
        "\n",
        "4. **<span style=\"color: blue;\">Special Case Handling</span>**:\n",
        "   - The function includes a special case where if the base word ends with 'i' and the original word ends with 'ness', the 'i' is replaced with 'y'.\n",
        "   - This handles words like \"happiness\" where the base form is \"happy\".\n",
        "\n",
        "5. **<span style=\"color: blue;\">Output</span>**:\n",
        "   - Finally, the function prints the resulting base word after processing the prefixes, suffixes, and special cases.\n"
      ],
      "metadata": {
        "id": "Qx6qoXTJ7O6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def solve(word):\n",
        "    prefixes = ['un', 're', 'in', 'dis', 'im', 'ir', 'il']\n",
        "    suffixes = ['ed', 'ing', 'ly', 'ness', 's', 'er', 'able', 'ment']\n",
        "\n",
        "    base_word = word\n",
        "\n",
        "    for prefix in prefixes:\n",
        "        if word.startswith(prefix):\n",
        "            base_word = word[len(prefix):]\n",
        "            print(f\"Prefix: {prefix}\")\n",
        "            break\n",
        "\n",
        "    for suffix in suffixes:\n",
        "        if base_word.endswith(suffix):\n",
        "            base_word = base_word[:-len(suffix)]\n",
        "            print(f\"Suffix: {suffix}\")\n",
        "            break\n",
        "\n",
        "    if base_word.endswith('i') and word.endswith('ness'):\n",
        "        base_word = base_word[:-1] + 'y'\n",
        "\n",
        "    print(f\"Base Word: {base_word}\")\n",
        "\n",
        "# Example usage\n",
        "solve(\"unhappiness\")"
      ],
      "metadata": {
        "id": "JWPC2TczmrHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966d8dcd-0c39-4bad-a91a-c569f9295d56"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prefix: un\n",
            "Suffix: ness\n",
            "Base Word: happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q2) Write a program that identifies the derivational morphemes in a given sentence and classifies each word into its base form and derivational affixes.**\n",
        "\n",
        "Example:\n",
        "Input Sentence: \"The artistically gifted boy performed exceptionally in the competition.\"\n",
        "\n",
        "Output:\n",
        "\n",
        "    \"word\": \"artistically\",\n",
        "    \"base_form\": \"art\",\n",
        "    \"derivational_affixes\": [\"-ist\", \"-ic\", \"-ally\"],\n",
        "  \n",
        "    \"word\": \"gifted\",\n",
        "    \"base_form\": \"gift\",\n",
        "    \"derivational_affixes\": [\"-ed\"],\n",
        "  \n",
        "    \"word\": \"performed\",\n",
        "    \"base_form\": \"perform\",\n",
        "    \"derivational_affixes\": [\"-ed\"],\n",
        "  \n",
        "    \"word\": \"exceptionally\",\n",
        "    \"base_form\": \"except\",\n",
        "    \"derivational_affixes\": [\"-ion\", \"-al\", \"-ly\"],\n",
        "  \n",
        "    \"word\": \"competition\",\n",
        "    \"base_form\": \"compete\",\n",
        "    \"derivational_affixes\": [\"-ition\"]\n"
      ],
      "metadata": {
        "id": "Avb1UNkPme6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the `DerivationalMorphemeAnalyzer` Class\n",
        "\n",
        "1. **<span style=\"color: blue;\">Initialization</span>**:\n",
        "   - The class `DerivationalMorphemeAnalyzer` initializes with a `WordNetLemmatizer` from NLTK for lemmatization.\n",
        "   - It defines lists of common <span style=\"color: green;\">prefixes</span> and <span style=\"color: green;\">suffixes</span> used for derivational morphology.\n",
        "\n",
        "2. **<span style=\"color: blue;\">Word Analysis</span>**:\n",
        "   - The `analyze_word` method processes a word to identify and remove prefixes and suffixes.\n",
        "   - **Prefix Removal**: Checks if the word starts with any prefix from the `prefixes` list and removes it if found.\n",
        "   - **Suffix Removal**: Iteratively checks if the word ends with any suffix from the `suffixes` list and removes it if found.\n",
        "   - The method uses the `WordNetLemmatizer` to obtain the base form of the word, which is returned along with identified affixes.\n",
        "\n",
        "3. **<span style=\"color: blue;\">Sentence Analysis</span>**:\n",
        "   - The `analyze_sentence` method tokenizes the input sentence into words.\n",
        "   - It then applies `analyze_word` to each word (excluding words shorter than 3 characters) and collects the results.\n",
        "\n",
        "4. **<span style=\"color: blue;\">Example Usage</span>**:\n",
        "   - An instance of `DerivationalMorphemeAnalyzer` is created.\n",
        "   - The `analyze_sentence` method is called with the input sentence: `\"The artistically gifted boy performed exceptionally in the competition.\"`\n",
        "   - The results are printed, showing each word's original form, its base form, and any derivational affixes identified.\n",
        "\n",
        "5. **<span style=\"color: blue;\">Output</span>**:\n",
        "   - The output will include detailed information for each word in the sentence, displaying the word's base form and any affixes removed, providing insight into the word's derivational morphology.\n"
      ],
      "metadata": {
        "id": "CaGhoAH17aFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "class DerivationalMorphemeAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.derivational_affixes = {\n",
        "            'prefixes': ['un', 're', 'dis', 'over', 'under', 'pre', 'post', 'non', 'anti', 'semi', 'mis'],\n",
        "            'suffixes': ['ness', 'less', 'ful', 'able', 'ible', 'al', 'ial', 'ic', 'ical', 'ed', 'en', 'er', 'est',\n",
        "                         'ify', 'ize', 'ise', 'ing', 'ion', 'ation', 'ition', 'ity', 'ty', 'ous', 'ious', 'ive',\n",
        "                         'ative', 'itive', 'ly', 'ward', 'wise', 'ist', 'ism', 'ship', 'able', 'ible', 'ary', 'ory',\n",
        "                         'ery', 'ally']\n",
        "        }\n",
        "\n",
        "    def analyze_word(self, word):\n",
        "        original_word = word\n",
        "        base_form = self.lemmatizer.lemmatize(word)\n",
        "        derivational_affixes = []\n",
        "\n",
        "        for prefix in self.derivational_affixes['prefixes']:\n",
        "            if word.startswith(prefix):\n",
        "                derivational_affixes.append(prefix)\n",
        "                word = word[len(prefix):]\n",
        "                break\n",
        "\n",
        "        while True:\n",
        "            found_suffix = False\n",
        "            for suffix in self.derivational_affixes['suffixes']:\n",
        "                if word.endswith(suffix) and len(word) > len(suffix):\n",
        "                    derivational_affixes.append(suffix)\n",
        "                    word = word[:-len(suffix)]\n",
        "                    found_suffix = True\n",
        "                    break\n",
        "            if not found_suffix:\n",
        "                break\n",
        "\n",
        "        if not derivational_affixes:\n",
        "            base_form = word\n",
        "\n",
        "        return {\n",
        "            \"word\": original_word,\n",
        "            \"base_form\": base_form,\n",
        "            \"derivational_affixes\": derivational_affixes\n",
        "        }\n",
        "\n",
        "    def analyze_sentence(self, sentence):\n",
        "        words = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
        "        return [self.analyze_word(word) for word in words if len(word) > 2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veigY4DzH7I9",
        "outputId": "8c783740-55fd-43a5-de19-e227ff906f0e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "analyzer = DerivationalMorphemeAnalyzer()\n",
        "input_sentence = \"The artistically gifted boy performed exceptionally in the competition.\"\n",
        "result = analyzer.analyze_sentence(input_sentence)\n",
        "\n",
        "for word_analysis in result:\n",
        "    print(word_analysis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p6iieEG7XRp",
        "outputId": "3a71d0f1-5d96-4138-bee6-b5c7d0e46b08"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'word': 'the', 'base_form': 'the', 'derivational_affixes': []}\n",
            "{'word': 'artistically', 'base_form': 'artistically', 'derivational_affixes': ['ly', 'al', 'ic', 'ist']}\n",
            "{'word': 'gifted', 'base_form': 'gifted', 'derivational_affixes': ['ed']}\n",
            "{'word': 'boy', 'base_form': 'boy', 'derivational_affixes': []}\n",
            "{'word': 'performed', 'base_form': 'performed', 'derivational_affixes': ['ed']}\n",
            "{'word': 'exceptionally', 'base_form': 'exceptionally', 'derivational_affixes': ['ly', 'al', 'ion']}\n",
            "{'word': 'the', 'base_form': 'the', 'derivational_affixes': []}\n",
            "{'word': 'competition', 'base_form': 'competition', 'derivational_affixes': ['ion']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q3) Implement a word segmentation program that can handle ambiguous cases where multiple segmentations are possible. The program should use a probabilistic model to choose the most likely segmentation.**\n",
        "Example Input: \"therapistfinder\"\n",
        "\n",
        "Example Output: [\"therapist\", \"finder\"] or [\"the\", \"rapist\", \"finder\"] (depending on context)"
      ],
      "metadata": {
        "id": "dR657un_o4sD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "medical_txt='''Dr. Emily Thompson is a renowned therapist who specializes in finding innovative solutions for mental health issues. She has been helping patients with anxiety and depression for over two decades. Her practice, called \"Therapist Finder,\" aims to connect individuals with the right mental health professionals. Dr. Thompson's approach includes personalized therapy plans and advanced techniques to ensure the best outcomes for her patients. With a team of skilled therapists, the clinic offers various services to support mental well-being and overall health.\n",
        "'''"
      ],
      "metadata": {
        "id": "MJLMn-qwi1Rj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crime_txt= '''\n",
        "Detective John Carter is investigating a series of serious crimes in the city. The latest case involves a crime scene where a notorious criminal is suspected of committing a rape act. The \"Finder\" team, a specialized police unit, is working diligently to gather evidence and track down the rapist. The investigation includes interviewing witnesses, analyzing forensic evidence, and following leads to apprehend the rapist. Detective Carter is determined to solve the case and bring justice to the raped victims.\n",
        "'''"
      ],
      "metadata": {
        "id": "G3Y0Kvz9i9Ba"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "def preprocess_text(text):\n",
        "    # Convert to lower case\n",
        "    text = text.lower()\n",
        "    # Remove non-alphanumeric characters except for spaces\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def generate_one_gram_file(text, output_file):\n",
        "    # Preprocess the text\n",
        "    processed_text = preprocess_text(text)\n",
        "    # Tokenize the text\n",
        "    words = processed_text.split()\n",
        "    # Count the frequency of each word\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    # Write to output file\n",
        "    with open(output_file, 'w') as file:\n",
        "        for word, count in word_counts.items():\n",
        "            file.write(f\"{word}\\t{count}\\n\")\n"
      ],
      "metadata": {
        "id": "gtGPGxzvjUIp"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate one-gram file for the crime text\n",
        "generate_one_gram_file(crime_txt, 'crime-grams.txt')\n",
        "\n",
        "# Generate one-gram file for the medical text\n",
        "generate_one_gram_file(medical_txt, 'medical-grams.txt')"
      ],
      "metadata": {
        "id": "w967N0E1jV35"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools, math\n",
        "\n",
        "class OneGramDist(dict):\n",
        "   def __init__(self, filename):\n",
        "      self.gramCount = 0\n",
        "\n",
        "      for line in open(filename):\n",
        "         (word, count) = line[:-1].split('\\t')\n",
        "         self[word] = int(count)\n",
        "         self.gramCount += self[word]\n",
        "\n",
        "   def __call__(self, key):\n",
        "      if key in self:\n",
        "         return float(self[key]) / self.gramCount\n",
        "      else:\n",
        "         return 1.0 / (self.gramCount * 10**(len(key)-2))\n"
      ],
      "metadata": {
        "id": "Ggx-Vg3yKYN_"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "singleWordProb = OneGramDist('crime-grams.txt')\n",
        "def wordSeqFitness(words):\n",
        "   return sum(math.log10(singleWordProb(w)) for w in words)"
      ],
      "metadata": {
        "id": "EHSEzypiK9WE"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def memoize(f):\n",
        "   cache = {}\n",
        "\n",
        "   def memoizedFunction(*args):\n",
        "      if args not in cache:\n",
        "         cache[args] = f(*args)\n",
        "      return cache[args]\n",
        "\n",
        "   memoizedFunction.cache = cache\n",
        "   return memoizedFunction\n",
        "\n",
        "@memoize\n",
        "def segment(word):\n",
        "   if not word: return []\n",
        "   word = word.lower() # change to lower case\n",
        "   allSegmentations = [[first] + segment(rest) for (first,rest) in splitPairs(word)]\n",
        "   return max(allSegmentations, key = wordSeqFitness)\n",
        "\n",
        "def splitPairs(word, maxLen=20):\n",
        "   return [(word[:i+1], word[i+1:]) for i in range(max(len(word), maxLen))]\n",
        "\n",
        "@memoize\n",
        "def segmentWithProb(word):\n",
        "   segmented = segment(word)\n",
        "   return (wordSeqFitness(segmented), segmented)"
      ],
      "metadata": {
        "id": "DKTCjkDiaylo"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "word = \"therapistfinder\"\n",
        "score, segmented = segmentWithProb(word)\n",
        "print(\"Score:\", score)\n",
        "print(\"Segmentation:\", segmented)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rCU-GXMgKRZ",
        "outputId": "04fb5740-5b93-45a8-bab1-2302be0f397e"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: -4.488761291215399\n",
            "Segmentation: ['the', 'rapist', 'finder']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "singleWordProb2 = OneGramDist('medical-grams.txt')\n",
        "def wordSeqFitness(words):\n",
        "   return sum(math.log10(singleWordProb(w)) for w in words)\n"
      ],
      "metadata": {
        "id": "dQuzkMiTkNM7"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def memoize(f):\n",
        "   cache = {}\n",
        "\n",
        "   def memoizedFunction(*args):\n",
        "      if args not in cache:\n",
        "         cache[args] = f(*args)\n",
        "      return cache[args]\n",
        "\n",
        "   memoizedFunction.cache = cache\n",
        "   return memoizedFunction\n",
        "\n",
        "@memoize\n",
        "def segment(word):\n",
        "   if not word: return []\n",
        "   word = word.lower() # change to lower case\n",
        "   allSegmentations = [[first] + segment(rest) for (first,rest) in splitPairs(word)]\n",
        "   return max(allSegmentations, key = wordSeqFitness)\n",
        "\n",
        "def splitPairs(word, maxLen=20):\n",
        "   return [(word[:i+1], word[i+1:]) for i in range(max(len(word), maxLen))]\n",
        "\n",
        "@memoize\n",
        "def segmentWithProb(word):\n",
        "   segmented = segment(word)\n",
        "   return (wordSeqFitness(segmented), segmented)"
      ],
      "metadata": {
        "id": "PQqSDAVfkOZ_"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "word = \"therapistfinder\"\n",
        "score, segmented = segmentWithProb(word)\n",
        "print(\"Score:\", score)\n",
        "print(\"Segmentation:\", segmented)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxIZnEfekOdU",
        "outputId": "9cff851b-3c8d-4068-d3bc-2042284a8b6f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: -3.5159400420933182\n",
            "Segmentation: ['therapist', 'finder']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q4) Write a program that uses a bigram language model to predict the next word in a given context. The program should consider the previous word to predict the next word with the highest probability.**\n",
        "Task: Train a bigram model on a text corpus and predict the next word for a given input word.\n",
        "\n",
        "**Example:**\n",
        "Once upon a time there was a brave knight.\n",
        "\n",
        "The knight was known for his bravery and courage.\n",
        "\n",
        "He fought many battles and won the hearts of the people.\n",
        "\n",
        "The people of the kingdom loved the knight.\n",
        "\n",
        "One day, the knight embarked on a journey to defeat a dragon.\n",
        "\n",
        "The dragon was terrorizing the villages in the kingdom.\n",
        "\n",
        "After a long and fierce battle, the knight defeated the dragon.\n",
        "\n",
        "The people celebrated the victory of the knight.\n",
        "\n",
        "The knight became a legend in the kingdom.\n",
        "\n",
        "\n",
        "**what is most probable next word predicted by the modal?**\n",
        "\n",
        "\"The knight\"\n",
        "\n",
        "\"The people\"\n",
        "\n",
        "\"The dragon\"\n",
        "\n",
        "\"A brave\"\n",
        "\n",
        "\"The kingdom\"\n",
        "\n",
        "**Expected Predictions:**\n",
        "For \"The knight\", the model might predict \"was\".\n",
        "\n",
        "For \"The people\", it might predict \"of\" or \"celebrated\".\n",
        "\n",
        "For \"The dragon\", it could predict \"was\" or \"defeated\".\n",
        "\n",
        "For \"A brave\", it might predict \"knight\".\n",
        "\n",
        "For \"The kingdom\", it could predict \"loved\" or \"celebrated\"."
      ],
      "metadata": {
        "id": "Vk92vz3bVWeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = \"\"\"\n",
        "Once upon a time there was a brave knight.\n",
        "The knight was known for his bravery and courage.\n",
        "He fought many battles and won the hearts of the people.\n",
        "The people of the kingdom loved the knight.\n",
        "One day, the knight embarked on a journey to defeat a dragon.\n",
        "The dragon was terrorizing the villages in the kingdom.\n",
        "After a long and fierce battle, the knight defeated the dragon.\n",
        "The people celebrated the victory of the knight.\n",
        "The knight became a legend in the kingdom.\n",
        "\"\"\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lower case\n",
        "    # Remove punctuation and tokenize\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words\n",
        "\n",
        "def train_bigram_model(words):\n",
        "    bigram_counts = Counter()\n",
        "    unigram_counts = Counter(words)\n",
        "\n",
        "    for i in range(len(words) - 1):\n",
        "        bigram = (words[i], words[i+1])\n",
        "        bigram_counts[bigram] += 1\n",
        "\n",
        "    return bigram_counts, unigram_counts\n",
        "\n",
        "def calculate_probabilities(bigram_counts, unigram_counts):\n",
        "    probabilities = defaultdict(dict)\n",
        "\n",
        "    for (word1, word2), count in bigram_counts.items():\n",
        "        probabilities[word1][word2] = count / unigram_counts[word1]\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "def predict_next_word(context, probabilities):\n",
        "    context = context.lower()\n",
        "    words = context.split()\n",
        "    if len(words) == 0:\n",
        "        return None\n",
        "\n",
        "    previous_word = words[-1]\n",
        "\n",
        "    if previous_word not in probabilities:\n",
        "        return None\n",
        "\n",
        "    next_words = probabilities[previous_word]\n",
        "    predicted_word = max(next_words, key=next_words.get)\n",
        "    return predicted_word\n",
        "\n",
        "# Process corpus\n",
        "words = preprocess_text(corpus)\n",
        "bigram_counts, unigram_counts = train_bigram_model(words)\n",
        "probabilities = calculate_probabilities(bigram_counts, unigram_counts)\n",
        "\n",
        "# Predict next word\n",
        "contexts = [\n",
        "    \"The knight\",\n",
        "    \"The people\",\n",
        "    \"The dragon\",\n",
        "    \"A brave\",\n",
        "    \"The kingdom\"\n",
        "]\n",
        "\n",
        "for context in contexts:\n",
        "    predicted_word = predict_next_word(context, probabilities)\n",
        "    print(f\"Context: '{context}' -> Predicted next word: '{predicted_word}'\")\n"
      ],
      "metadata": {
        "id": "D4aJ3JtOcIu7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8a96dd-1928-4a99-8d85-20ce2fff053e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: 'The knight' -> Predicted next word: 'the'\n",
            "Context: 'The people' -> Predicted next word: 'the'\n",
            "Context: 'The dragon' -> Predicted next word: 'the'\n",
            "Context: 'A brave' -> Predicted next word: 'knight'\n",
            "Context: 'The kingdom' -> Predicted next word: 'loved'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = \"\"\"\n",
        "Once upon a time there was a brave knight.\n",
        "The knight was known for his bravery and courage.\n",
        "He fought many battles and won the hearts of the people.\n",
        "The people of the kingdom loved the knight.\n",
        "One day, the knight embarked on a journey to defeat a dragon.\n",
        "The dragon was terrorizing the villages in the kingdom.\n",
        "After a long and fierce battle, the knight defeated the dragon.\n",
        "The people celebrated the victory of the knight.\n",
        "The knight became a legend in the kingdom.\n",
        "\"\"\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lower case\n",
        "    # Remove punctuation and tokenize\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words\n",
        "\n",
        "def train_trigram_model(words):\n",
        "    trigram_counts = Counter()\n",
        "    bigram_counts = Counter()\n",
        "    unigram_counts = Counter(words)\n",
        "\n",
        "    for i in range(len(words) - 2):\n",
        "        trigram = (words[i], words[i+1], words[i+2])\n",
        "        bigram = (words[i], words[i+1])\n",
        "        trigram_counts[trigram] += 1\n",
        "        bigram_counts[bigram] += 1\n",
        "\n",
        "    return trigram_counts, bigram_counts, unigram_counts\n",
        "\n",
        "def calculate_trigram_probabilities(trigram_counts, bigram_counts):\n",
        "    probabilities = defaultdict(dict)\n",
        "\n",
        "    for (word1, word2, word3), count in trigram_counts.items():\n",
        "        bigram = (word1, word2)\n",
        "        probabilities[bigram][word3] = count / bigram_counts[bigram]\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "def predict_next_word(context, probabilities):\n",
        "    context = context.lower()\n",
        "    words = context.split()\n",
        "\n",
        "    if len(words) < 2:\n",
        "        return None\n",
        "\n",
        "    bigram = (words[-2], words[-1])\n",
        "\n",
        "    if bigram not in probabilities:\n",
        "        return None\n",
        "\n",
        "    next_words = probabilities[bigram]\n",
        "    predicted_word = max(next_words, key=next_words.get)\n",
        "    return predicted_word\n",
        "\n",
        "# Process corpus\n",
        "words = preprocess_text(corpus)\n",
        "trigram_counts, bigram_counts, unigram_counts = train_trigram_model(words)\n",
        "probabilities = calculate_trigram_probabilities(trigram_counts, bigram_counts)\n",
        "\n",
        "# Predict next word\n",
        "contexts = [\n",
        "    \"The knight\",\n",
        "    \"The people\",\n",
        "    \"The dragon\",\n",
        "    \"A brave\",\n",
        "    \"The kingdom\"\n",
        "]\n",
        "\n",
        "for context in contexts:\n",
        "    predicted_word = predict_next_word(context, probabilities)\n",
        "    print(f\"Context: '{context}' -> Predicted next word: '{predicted_word}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwXOVnAoukqs",
        "outputId": "bfd14f88-c4a7-44f7-de5c-479889cbedbf"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: 'The knight' -> Predicted next word: 'was'\n",
            "Context: 'The people' -> Predicted next word: 'the'\n",
            "Context: 'The dragon' -> Predicted next word: 'was'\n",
            "Context: 'A brave' -> Predicted next word: 'knight'\n",
            "Context: 'The kingdom' -> Predicted next word: 'loved'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q5) Write a program that implements an N-gram (e.g., trigram) language model. The program should calculate the probability of a given sequence of words.**\n",
        "Task: Implement a trigram model using a provided text corpus. Calculate the probability of a given sentence.\n",
        "\n",
        "Corpus:\n",
        "\n",
        "I am a human.\n",
        "\n",
        "I am not a stone.\n",
        "\n",
        "I I live in Mumbai.\n",
        "\n",
        "Check the probability of \" I I am not \"\n",
        "\n"
      ],
      "metadata": {
        "id": "W8nNCRrScJKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = \"\"\"\n",
        "I am a human.\n",
        "I am not a stone.\n",
        "I I live in Mumbai.\n",
        "\"\"\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lower case\n",
        "    # Remove punctuation and tokenize\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words\n",
        "\n",
        "def train_trigram_model(words):\n",
        "    trigram_counts = Counter()\n",
        "    bigram_counts = Counter()\n",
        "    unigram_counts = Counter(words)\n",
        "\n",
        "    for i in range(len(words) - 2):\n",
        "        trigram = (words[i], words[i+1], words[i+2])\n",
        "        bigram = (words[i], words[i+1])\n",
        "        trigram_counts[trigram] += 1\n",
        "        bigram_counts[bigram] += 1\n",
        "\n",
        "    return trigram_counts, bigram_counts, unigram_counts\n",
        "\n",
        "def calculate_trigram_probabilities(trigram_counts, bigram_counts):\n",
        "    probabilities = defaultdict(dict)\n",
        "\n",
        "    for (word1, word2, word3), count in trigram_counts.items():\n",
        "        bigram = (word1, word2)\n",
        "        probabilities[bigram][word3] = count / bigram_counts[bigram]\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "def calculate_sentence_probability(sentence, probabilities):\n",
        "    words = sentence.lower().split()\n",
        "    if len(words) < 3:\n",
        "        return 0.0\n",
        "\n",
        "    probability = 1.0\n",
        "    for i in range(len(words) - 2):\n",
        "        bigram = (words[i], words[i+1])\n",
        "        next_word = words[i+2]\n",
        "        if bigram in probabilities and next_word in probabilities[bigram]:\n",
        "            probability *= probabilities[bigram][next_word]\n",
        "        else:\n",
        "            return 0.0  # Return 0 if any trigram is not found in the model\n",
        "\n",
        "    return probability\n"
      ],
      "metadata": {
        "id": "G-1JtgxHdXS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "828e6cc7-9ca7-411e-e2b4-53f9d4b3272a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of the sentence 'I I am not': 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Process corpus\n",
        "words = preprocess_text(corpus)\n",
        "trigram_counts, bigram_counts, unigram_counts = train_trigram_model(words)\n",
        "probabilities = calculate_trigram_probabilities(trigram_counts, bigram_counts)\n",
        "\n",
        "# Check the probability of the given sentence\n",
        "sentence = \"I I am not\"\n",
        "probability = calculate_sentence_probability(sentence, probabilities)\n",
        "print(f\"Probability of the sentence '{sentence}': {probability}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63J5b4-5vfOr",
        "outputId": "b6052a67-b23e-4a83-ced8-73d9cf4201f3"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of the sentence 'I I am not': 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q6) Write a program that applies smoothing techniques (e.g., Laplace, Good-Turing) to a bigram language model. Compare the performance of the model with and without smoothing.**\n",
        "\n",
        "Corpus:\n",
        "\n",
        "I love natural language processing.\n",
        "\n",
        "Language models are great.\n",
        "\n",
        "I love machine learning.\n",
        "\n",
        "Machine learning is fun.\n",
        "\n",
        "Natural language processing is a complex field.\n"
      ],
      "metadata": {
        "id": "jVTsuDc_dXi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = \"\"\"\n",
        "I love natural language processing.\n",
        "Language models are great.\n",
        "I love machine learning.\n",
        "Machine learning is fun.\n",
        "Natural language processing is a complex field.\n",
        "\"\"\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lower case\n",
        "    # Remove punctuation and tokenize\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words\n",
        "\n",
        "def train_bigram_model(words):\n",
        "    bigram_counts = Counter()\n",
        "    unigram_counts = Counter(words)\n",
        "\n",
        "    for i in range(len(words) - 1):\n",
        "        bigram = (words[i], words[i+1])\n",
        "        bigram_counts[bigram] += 1\n",
        "\n",
        "    return bigram_counts, unigram_counts\n",
        "\n",
        "def laplace_smoothing(bigram_counts, unigram_counts, vocabulary_size):\n",
        "    smoothed_probabilities = defaultdict(dict)\n",
        "\n",
        "    for (word1, word2), count in bigram_counts.items():\n",
        "        smoothed_probabilities[word1][word2] = (count + 1) / (unigram_counts[word1] + vocabulary_size)\n",
        "\n",
        "    return smoothed_probabilities\n",
        "\n",
        "def good_turing_smoothing(bigram_counts, unigram_counts):\n",
        "    bigram_count_freq = Counter(bigram_counts.values())\n",
        "    total_bigrams = sum(bigram_count_freq.values())\n",
        "    smoothed_probabilities = defaultdict(dict)\n",
        "\n",
        "    for (word1, word2), count in bigram_counts.items():\n",
        "        next_count = bigram_count_freq[count + 1] if count + 1 in bigram_count_freq else 0\n",
        "        smoothed_probabilities[word1][word2] = (count + 1) * next_count / (total_bigrams * (bigram_count_freq[count] or 1))\n",
        "\n",
        "    return smoothed_probabilities\n",
        "\n",
        "def calculate_probability(sentence, probabilities, vocabulary_size=None):\n",
        "    words = sentence.lower().split()\n",
        "    if len(words) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    probability = 1.0\n",
        "    for i in range(len(words) - 1):\n",
        "        bigram = (words[i], words[i+1])\n",
        "        if bigram in probabilities:\n",
        "            next_word_prob = probabilities[bigram[0]].get(bigram[1], 1 / (vocabulary_size if vocabulary_size else 1))\n",
        "            probability *= next_word_prob\n",
        "        else:\n",
        "            return 0.0  # Return 0 if any bigram is not found in the model\n",
        "\n",
        "    return probability\n",
        "\n",
        "# Process corpus\n",
        "words = preprocess_text(corpus)\n",
        "bigram_counts, unigram_counts = train_bigram_model(words)\n",
        "vocabulary_size = len(set(words))\n",
        "\n",
        "# Apply smoothing techniques\n",
        "laplace_probabilities = laplace_smoothing(bigram_counts, unigram_counts, vocabulary_size)\n",
        "good_turing_probabilities = good_turing_smoothing(bigram_counts, unigram_counts)\n",
        "\n",
        "# Calculate probabilities for a given sentence\n",
        "sentence = \"I love machine learning.\"\n",
        "\n",
        "unsmoothed_probability = calculate_probability(sentence, defaultdict(lambda: defaultdict(float)))\n",
        "laplace_probability = calculate_probability(sentence, laplace_probabilities, vocabulary_size)\n",
        "good_turing_probability = calculate_probability(sentence, good_turing_probabilities)\n",
        "\n",
        "print(f\"Unsmoothed Probability: {unsmoothed_probability}\")\n",
        "print(f\"Laplace Smoothed Probability: {laplace_probability}\")\n",
        "print(f\"Good-Turing Smoothed Probability: {good_turing_probability}\")\n"
      ],
      "metadata": {
        "id": "phFEP9aGdcMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "541ac4ff-dc2e-4bdc-ad50-250e473f23dd"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsmoothed Probability: 0.0\n",
            "Laplace Smoothed Probability: 0.0\n",
            "Good-Turing Smoothed Probability: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q7) Write a program that implements a simple rule-based POS tagger. The program should take a sentence as input and assign POS tags based on predefined rules. For instance, you might define rules such as:**\n",
        "\n",
        "Words ending in \"ed\" are tagged as verbs (e.g., \"walked\" → VBD).\n",
        "\n",
        "Words ending in \"ly\" are tagged as adverbs (e.g., \"quickly\" → RB).\n",
        "Example:\n",
        "\n",
        "Input: \"The cat quickly jumped over the lazy dog.\"\n",
        "\n",
        "Output: [('The', 'DT'), ('cat', 'NN'), ('quickly', 'RB'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]"
      ],
      "metadata": {
        "id": "TJd9yLSVjGPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def pos_tagging(sentence):\n",
        "    # Define the rules for tagging\n",
        "    rules = {\n",
        "        'DT': ['the', 'a', 'an'],  # Determiners\n",
        "        'NN': lambda word: re.search(r'\\b(cat|dog|field|house)\\b', word),\n",
        "        'RB': lambda word: word.endswith('ly'),\n",
        "        'VBD': lambda word: word.endswith('ed'),\n",
        "        'IN': ['over', 'under', 'in', 'on', 'at'],\n",
        "        'JJ': lambda word: re.search(r'\\b(lazy|quick|happy)\\b', word),\n",
        "    }\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    words = sentence.split()\n",
        "\n",
        "    tagged_words = []\n",
        "    for word in words:\n",
        "        word_lower = word.lower()\n",
        "        tag_found = False\n",
        "\n",
        "        for tag, criteria in rules.items():\n",
        "            if isinstance(criteria, list):\n",
        "                if word_lower in criteria:\n",
        "                    tagged_words.append((word, tag))\n",
        "                    tag_found = True\n",
        "                    break\n",
        "            elif callable(criteria):\n",
        "                if criteria(word_lower):\n",
        "                    tagged_words.append((word, tag))\n",
        "                    tag_found = True\n",
        "                    break\n",
        "\n",
        "        if not tag_found:\n",
        "            tagged_words.append((word, 'NN'))\n",
        "\n",
        "    return tagged_words\n",
        "\n",
        "sentence = \"The cat quickly jumped over the lazy dog.\"\n",
        "tagged_sentence = pos_tagging(sentence)\n",
        "\n"
      ],
      "metadata": {
        "id": "iDgU-ygCjSl9"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_sentence"
      ],
      "metadata": {
        "id": "YMGHH9uXn-fx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54be65e8-ce64-4f6e-a945-328ec0be9a97"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('cat', 'NN'),\n",
              " ('quickly', 'RB'),\n",
              " ('jumped', 'VBD'),\n",
              " ('over', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('lazy', 'JJ'),\n",
              " ('dog.', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1H4TpnFFv5f6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}